\documentclass[10pt,reqno,final]{amsart}
\input{header}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,decorations,calc,math}

\title[Exact Phylodynamics]{Exact Phylodynamics via Structured Markov Genealogy Processes}
\author[King]{Aaron~A.~King}
\address{
  A.~A.~King,
  Department of Ecology \& Evolutionary~Biology,
  Center for the Study of Complex~Systems, and
  Department of Mathematics,
  University of Michigan,
  Ann~Arbor, MI~48109~USA\\
  Santa~Fe~Institute,
  1399 Hyde~Park~Road,
  Santa~Fe, NM~87501~USA
}
\email{kingaa@umich.edu}
\urladdr{\href{https://kinglab.eeb.lsa.umich.edu/}{https://kinglab.eeb.lsa.umich.edu/}}
\author[Lin]{Qianying~Lin}
\address{
  Q.-Y.~Lin,
  Theoretical Biology and Biophysics,
  Los~Alamos National Laboratory,
  Los~Alamos, NM~87545~USA
}
\author[Ionides]{Edward~L.~Ionides}
\address{
  E.~L.~Ionides,
  Department of Statistics
  University of Michigan,
  Ann~Arbor, MI~48109~USA
}
\date{\today}

\hypersetup{pdftitle={Exact Phylodynamics via Structured Markov Genealogy Processes}}
\hypersetup{pdfauthor={A.A. King, Q.-Y. Lin, E.L. Ionides}}
\hypersetup{urlcolor=blue,citecolor=blue,linkcolor=blue,filecolor=blue}

<<prefix,include=FALSE,cache=FALSE,purl=FALSE>>=
prefix <- "sgp"
source("setup.R")
@
<<packages,include=FALSE,cache=FALSE>>=
library(tidyverse)
library(ggtree)
library(pomp)
library(cowplot)
library(phylopomp)
stopifnot(getRversion() >= "4.3")
stopifnot(packageVersion("pomp")>="5.1")
stopifnot(packageVersion("phylopomp")>="0.9.2")
theme_set(theme_bw(base_family="serif"))
set.seed(1159254136)
@

\begin{document}

\begin{abstract}
  We derive an algorithm for the exact computation of the likelihood of an observed transmission tree.
\end{abstract}

\maketitle

\section{Introduction}

Problem of phylodynamics.
Factorization of problem into two subproblems.

Relation to previous work.
Existing methods \citep{Volz2009a,Stadler2010}.
Large-population, small sample-size approximations.

Extension of previous results \citep{King2022}.
Broader class of state-spaces.
Accommodating discrete structure.

Classes of Markov processes.
Utility and flexibility of Markov assumptions.

Population process induces Markov history and genealogy processes.
Using these, we derive equations for the likelihood of a genealogy conditional on the history.
We then integrate out the history to obtain nonlinear filtering equations, the solution of which yields the likelihood.
These readily lend themselves to a family of sequential Monte Carlo algorithms for computing the likelihood.
We demonstrate with several examples.

In the following, we show a Markov population process of the kind that is a staple in epidemiology induces a Markov process on the space of genealogies.
We then show how one can comput the likelihood of a given genealogy.

\section{Mathematical preliminaries}

\subsection{Notation}

Throughout the paper, we will adopt the convention that a bold-face symbol (\eg $\Xr$), denotes a random element.
We will be concerned with a variety of stochastic processes, in both discrete and continuous time.
In both cases, we will use a subscript to indicate the time parameter: \eg $\Xr_t$ or $Gr_k$, where $t$ takes values in the non-negative reals $\Rp$ and $k$ in the non-negative integers $\Zp$.
In the case of continuous-time processes, we will assume that sample paths are right-continuous with left limits (\ie \cadlag).
We will frequently need to refer to the left-limit of such a process.
Accordingly, if $\Zr_t$ is a \cadlag\ random process, we define
\begin{equation*}
  \Zrt_t\colonequals
  \begin{cases}
    \displaystyle\lim_{t'\,\uparrow\,{t}}\;\Zr_{t'}, & t>0,\\[2ex]
    \Zr_0, & t=0.\\
  \end{cases}
\end{equation*}
Note that $\Zrt_t$ is thus left-continuous with right limits.

If $Zr_t$, $t\in\Rp$ is a pure jump process, knowledge of its sample path is equivalent to knowledge of the number, $\Kr_t$, of jumps it has taken as of time $t$, the jump times $\Trh_k$, and the embedded chain $\Zrh_k\coloneq{\Zr_{\Trh_k}}$, $k=0,\dots,\Kr_t$.
In particular, if we adopt the convention that $\Trh_0=0$ and $\Trh_{\Kr_t+1}=t$, then
$\Zr_t=\Zrh_k$ for $t\in\halfopen{\Trh_k,\Trh_{k+1}}$, $k=0,\dots,\Kr_t$.
If $\Zr$ is such a pure jump process, let $\event{\Zr}{t}=\Set{\Trh_1,\dots,\Trh_{\Kr_t}}$ denote the set of its event times in the interval $[0,t]$.

\subsection{Population process}

Motivating examples: compartmental models.
Wide variety of models.
Linear chain trick.
Migration, superspreading, competition between strains.

\begin{figure}
  \begin{center}
    \resizebox{0.9\linewidth}{!}{
      \begin{tikzpicture}[scale=1]
        \tikzstyle{box}=[color=black, draw, fill=white, very thick, minimum size=3em]
        \tikzstyle{label}=[font=\Large]
        \tikzstyle{coordinate}=[inner sep=0pt,outer sep=0pt]
        \tikzstyle{flow}=[color=black, very thick, >=stealth]
        \tikzstyle{deme}=[fill=black!20!white]
        \tikzstyle{modulate}=[color=blue, thick, >=latex]
        \coordinate (origin) at (0,0);
        \node[label] (lab) at ($(origin)+(-0.5,2.1)$) {\textbf{A}};
        \input{figs/seir_diagram}
        \coordinate (origin) at (9,0);
        \node[label] (lab) at ($(origin)+(-0.5,2.1)$) {\textbf{B}};
        \input{figs/siir_diagram}
        \coordinate (origin) at (0,-4);
        \node[label] (lab) at ($(origin)+(-0.5,2.1)$) {\textbf{C}};
        \input{figs/covid_diagram}
      \end{tikzpicture}
    }
  \end{center}
  \caption{
    Examples of discretely-structured population models.
    Demes are shaded.
    (A) An SEIRS model.
    Susceptible individuals ($\lab{S}$), once infected, enter a transient incubation phase ($\lab{E}$) before they become infectious ($\lab{I}$).
    Upon recovery ($\lab{R}$), individuals experience immunity from reinfection.
    If this immunity wanes, they re-enter the susceptible compartment.
    Pathogen lineages are to be found in hosts within the $\lab{E}$ and $\lab{I}$ compartments only.
    Accordingly, there are two demes ($\Demes=\Set{\lab{E},\lab{I}}$).
    If there is exactly one lineage per host, then the occupancy, $n(\Xr_t)=(n_{\lab{E}}(\Xr_t),n_{\lab{I}}(\Xr_t))$, is the integer 2-vector giving the numbers of hosts in the respective compartments.
    \AAK{[Perhaps another one or two examples here?]}
    \AAK{[We could add dots to the deme compartments to signify individuals\dots.]}
    \label{fig:examples}
  }
\end{figure}

We will assume that our population process is a time-inhomogeneous Markov jump process, $\Xr_t$, $t\in\Rp$, taking values in some space $\Xspace$.
In earlier work \citep{King2022}, we limited ourselves to the case $\Xspace=\Z^d$, but here we assume only that $\Xspace$ is a complete metric space with a countable dense subset, \ie a Polish space.
The population process is completely specified by its initial-state distribution, $p_0$, and its transition rates $\alpha$.
In particular, we suppose that
\begin{equation}\label{eq:ic}
  \Prob{\Xr_0\in\mathcal{E}}=\int_{\mathcal{E}}{p_0(x)\,\dd{x}}
\end{equation}
for all measurable sets $\mathcal{E}\subseteq\Xspace$.
For any $t\in\Rp$, $x,x'\in\Xspace$, we think of the quantity $\alpha(t,x,x')$ as the instantaneous hazard of a jump from $x$ to $x'$.
More precisely, the transition rates have the following properties:
\begin{equation*}
  \begin{gathered}
    \alpha(t,x,x')\ge{0}, \qquad \int_{\Xspace}{\alpha(t,x,x')\,\dd{x'}}<\infty,\\
  \end{gathered}
\end{equation*}
for all $t\in\Rp$ and $x,x'\in\Xspace$.
Henceforth, we understand that integrals are taken over all of $\Xspace$ unless otherwise specified.
Let $\Kr_t$ be the number of jumps that $\Xr$ has taken by time $t$.
We assume that $\Kr_t$ is a simple counting process so that
\begin{equation*}
  \begin{gathered}
    \CondProb{\Kr_{t+\Delta}=n+1}{\Kr_{t}=n}=\Delta\,\int{\alpha(t,x,x')\,\dd{x'}}+o(\Delta),\\
    \CondProb{\Kr_{t+\Delta}>n+1}{\Kr_{t}=n}=o(\Delta),\\
    \CondProb{\Xr_{t+\Delta}\in\mathcal{E}}{\Xr_{t}=x, \Kr_{t+\Delta}-\Kr_{t}=1}=\frac{\int_{\mathcal{E}}{\alpha(t,x,x')\,\dd{x'}}}{\int{\alpha(t,x,x')\,\dd{x'}}}.
  \end{gathered}
\end{equation*}
We will further assume that $\Xr_t$ is non-explosive, \ie that $\Prob{\Kr_t<\infty}=1$ for all $t$.

\subsubsection{Kolmogorov forward equation}

The above may be compactly summarized by stating that if $w(t,x)$ satisfies the Kolmogorov forward equation (KFE),
\begin{equation}\label{eq:kfe}
  \frac{\partial{w}}{\partial{t}}(t,x)=\int\!w(t,x')\,\alpha(t,x',x)\,\dd{x'}-\int\!w(t,x)\,\alpha(t,x,x')\,\dd{x'},
\end{equation}
and if, moreover, $w(0,x) = p_0(x)$,
then $\int_{\mathcal{E}}\!{w(t,x)\,\dd{x}}=\Prob{\Xr_t\in\mathcal{E}}$ for every measurable $\mathcal{E}\subseteq{\Xspace}$.
\Cref{eq:kfe} is sometimes called the \emph{master equation} for $\Xr_t$.

Another perspective on the Markov processes is to be had from its Markov state transition diagram (\cref{fig:markov_state}).

\begin{figure}
  \begin{center}
    \resizebox{0.9\linewidth}{!}{
      \begin{tikzpicture}[scale=1]
        \usetikzlibrary{shapes,arrows,positioning}
        \tikzstyle{coordinate}=[inner sep=0pt,outer sep=0pt]
        \tikzstyle{state}=[shape=ellipse, color=black, draw, font=\LARGE, fill=white, thick, minimum size=5em]
        \tikzstyle{trans}=[color=black, thick, >=stealth]
        \node[state] (base) at (0,0) {$x=(S,E,I,R)$};
        \node[state] (trans) at (165:8.4) {$x'=(S-1,E+1,I,R)$};
        \node[state] (prog) at (195:8) {$x'=(S,E-1,I+1,R+1)$};
        \node[state] (recov) at (295:4) {$x'=(S,E,I-1,R+1)$};
        \node[state] (wane) at (355:9) {$x'=(S+1,E,I,R-1)$};
        \node[state] (sample) at (18:7.5) {$x'=(S,E,I,R)$};
        \draw[trans,->] (base) -- (trans) node[midway,above,sloped] {$\lab{Trans}$};
        \draw[trans,->] (base) -- (prog) node[midway,above,sloped] {$\lab{Prog}$};
        \draw[trans,->] (base) -- (recov) node[midway,above,sloped] {$\lab{Recov}$};
        \draw[trans,->] (base) -- (wane) node[midway,above,sloped] {$\lab{Wane}$};
        \draw[trans,->] (base) -- (sample) node[midway,above,sloped] {$\lab{Sample}$};
        \node[font=\Large] (U) at (95:3.2) {$\Jumps=\Set{\lab{Trans},\lab{Prog},\lab{Recov},\lab{Wane},\lab{Sample}}$};
      \end{tikzpicture}
    }
  \end{center}
  \caption{
    Markov state transition diagram for the SEIRS model depicted in \cref{fig:examples}A.
    The state, $x$, is characterized by four numbers, $S$, $E$, $I$, and $R$.
    From a given state $x$, there are five possible kinds of jumps $x\mapsto{x'}$.
    Accordingly, the set, $\Jumps$, of jump marks has five elements.
    Each of these is of a different type:
    $\lab{Trans}$ (transmission) is of birth type,
    $\lab{Prog}$ (progression) is of migration type,
    $\lab{Recov}$ (recovery) is of death type,
    $\lab{Sample}$ (sampling) is of sample type,
    and $\lab{Wane}$ (loss or waning of immunity) is of neutral type.
    See \cref{sec:jump_types} for a description of these jump types.
    Note that, in this formulation, when a sampling event occurs, the state does not change.
    \label{fig:markov_state}
  }
\end{figure}

\subsubsection{Structured populations, demes}

In an \emph{unstructured} Markov population process, every lineage is exactly like every other.
\citet{King2022} showed how every such process induces an unstructured Markov genealogy process.
Here, our aim is to expand the theory considerably by allowing our population of lineages to have discrete structure.
In particular, we suppose that there are a countable set of subpopulations that differ in their vital rates, but within each of which, individual lineages are statistically identical.
We call these subpopulations \emph{demes}, and use the symbol $\Demes$ to denote an index set for them.

For any $i\in\Demes$, we let $n_i(\Xr_t)$ denote the number of lineages present in deme $i$ at time $t$, \ie the \emph{occupancy} of deme $i$.
Thus $n(\Xr_t)\in\Zp^{\Demes}$ is the vector of deme occupancies.

\subsubsection{Jump marks}

In the following, it will be useful to break the jumps into distinct categories.
For this purpose, we let $\Jumps$ be a countable set of jump \emph{marks} such that
\begin{equation*}
  \alpha(t,x,x')=\sum_u{\alpha_u(t,x,x')}.
\end{equation*}
In \cref{fig:markov_state}, we use the marks to distinguish events of different types.
Here and in the following, sums over $u$ are taken over the whole of $\Jumps$ unless otherwise indicated.

Let us define the \emph{jump mark} process, $\Ur_t$, to be the mark of the latest jump as of time $t$.
As usual, we take the sample paths of $\Ur_t$ to be \cadlag.
Observe that $(\Xr_t,\Ur_t)$ is a Markov process, though $\Ur_t$ is not.

\subsection{Examples}

\subsubsection{SEIRS model}

\subsubsection{SIIR model}

\subsubsection{Linear birth-death model}

\subsubsection{Moran model and the Kingman coalescent}

\subsection{History process}

Consider the Markov process $(\Xr_t,\Ur_t)$.
We define its \emph{history process}, $\Hr_t$, to be the restriction of the random function $s\mapsto(\Xr_s,\Ur_s)$ to the interval $[0,t]$.
Note that $\Hr_t$ is itself trivially a Markov process, since it contains its own history.
Alternatively, one can think of $\Hr_t$ as consisting of the sequence
$\left(\left(\Trh_k,\Xrh_k,\Urh_k\right)\right)_{k=0}^{\Kr_t}$.
In particular, conditional on $\Hr_t$, both $\Xr_t$ and $\Ur_t$ are deterministic as are $\Kr_t$ and the embedded chains, $\Xrh_k$, $\Urh_k$, and the point process of event times $\Trh_k$.
The probability measure, $\pi^{\mathrm{H}}$, for $\Hr_t$ can be expressed in terms of these:
\begin{equation*}\label{eq:Hdens}
  \begin{aligned}
    \pi^{\mathrm{H}}(\dd{\H_t})
    =&p_{0}(\Xh_{0})\,\dd{\Xh_0}\,
    \prod_{k=1}^{K_{t}}{\alpha_{\Uh_k}\!\!\left(\Th_k,\Xh_{k-1},\Xh_{k}\right)\,\dd{\Xh_k}\,\dd{\Th_k}}\\
    &\qquad\times\exp{\left(-\sum_{k=0}^{K_t}{\int_{\Th_{k}}^{\Th_{k+1}}{\sum_{u}{\int{\alpha_{u}(t',\Xh_{k},x')\,\dd{x'}}}}\,\dd{t'}}\right)},
  \end{aligned}
\end{equation*}
where again, by convention, $\Th_0=0$ and $\Th_{K_t+1}=t$.

\subsection{Binomial ratio}\label{sec:binomial_ratio}

For $n,r,\ell,s\in{\Zp^\Demes}$, define the \emph{binomial ratio}
\begin{equation*}
  \BinRatio{n}{\ell}{r}{s}\colonequals
  \begin{cases}
    \frac{\displaystyle\prod_{i\in\Demes}{\binom{n_i-\ell_i}{r_i-s_i}}}%
         {\displaystyle\prod_{i\in\Demes}{\binom{n_i}{r_i}}},
         & \text{if}\ \forall i\ n_i\ge{\Set{\ell_i,r_i}}\ge{s_i}\ge{0},\\
         0, & \text{otherwise}.
  \end{cases}
\end{equation*}
Observe that $\BinRatio{n}{\ell}{r}{s}\in{[0,1]}$.
Moreover, in consequence of the Chu-Vandermonde identity, we have
\begin{equation*}
  \sum_{s\in\Zp^{\Demes}}\BinRatio{n}{\ell}{r}{s}\binom{\ell}{s}=1,
\end{equation*}
whenever $n_i\ge{\Set{\ell_i,r_i}}\ge{0}$ for all $i$.

\section{The induced genealogy process}

\subsection{Genealogies}

For our purposes, a \emph{genealogy}, $G_{T}$, is a labeled, time-calibrated tree.
It encapsulates the relationships of shared ancestry among a set of lineages that are alive at time $T\in\Rp$, and perhaps a set of samples collected at earlier times.
There are three distinct kinds of nodes:
\begin{inparaenum}[(i)]
\item \emph{tip nodes}, which represent labeled living lineages;
\item \emph{internal nodes}, which represent events at which lineages diverged and/or moved from one deme to another; and
\item \emph{sample nodes}, which represent labeled samples.
\end{inparaenum}
Each node $a$ is associated with a specific time, $t(a)$.
In particular, if $a$ is a tip node in $G_T$, then $t(a)=T$;
if $a$ is a sample node, then $t(a)$ is the time at which the sample was taken.
Moreover, if node $a$ is ancestral to node $a'$, then $t(a)\le{t(a')}$ and $t(a')-t(a)$ is the distance between $a$ and $a'$ along the genealogy.
We let $\event{G}{T}$ denote the set of all node-times of the genealogy $G_T$.
Importantly, a genealogy informs us not only about the shared ancestry of any pair of lineages, but also about where in the set of demes any given lineage was at all times.
Accordingly, we can visualize a genealogy as a tree, the nodes and edges of which are painted, with a distinct color for each deme.

Formally, we define a genealogy, $G_T$, to be an ordered pair $(y,z)$.
The $z$ element specifies the topological structure, as follows.
Let $\leaves{G_T}$ be the set of all tip and sample nodes in $G_T$ and define $\func{P}$ to be the set of all partitions of $\leaves{G_T}$.
The partition \emph{fineness} defines a partial order on $\func{P}$.
Specifically, for $p,p'\in{\func{P}}$, we say $p\preceq{p'}$ if and only if for every $c'\in{p'}$ there is a unique $c\in{p}$ such that $c\supseteq{c'}$.
The topological structure of $G_T$ is defined by a map $z:[0,T]\to\func{P}$ that is \cadlag\ and monotone in the sense that $t_1<t_2$ implies $\z_{t_1}\preceq{\z_{t_2}}$.

The $y$ element of $G_T$ specifies the coloring of branches and location of nodes.
In particular, for $a\in\leaves{G_T}$ and $t\in[0,T]$, define $d_{t}(a)$ to be the deme in which the lineage of $a$ was found at time $t$.
To make this well-defined, let $d_{t}(a)=\eth$ for $t\in{[t(a),T]}$;
we call $\eth$ the \emph{underdeme} and define $\Demesplus\coloneq{\Demes\cup\{\eth\}}$.
Thus $d:[0,T]\times\leaves{G_T}\to\Demesplus$ is a well-defined \cadlag\ function.

Since not every node along a lineage corresponds to a change of deme, we also count the number of nodes along each lineage.
Let $m:[0,T]\times\leaves{G_T}\to\Zp$ be such that $m_{t}(a)$ is the number of internal or sample nodes encountered along lineage $a$ as one goes from time $0$ to time $t\le{T}$ along the genealogy.
For $t\in{[t(a),T]}$, let $m_{t}(a)=m_{t(a)}(a)$.
Clearly, $m_t(a)$ is a simple counting process, with $m_0(a)=0$ for all $a$.

Finally, we define the map $y:[0,T]\times\leaves{G_T}\to\Demesplus\times\Zp$ by $y_{t}(a)=\left(d_{t}(a),m_{t}(a)\right)$.
The two \cadlag\ functions $y$ and $z$ specify the genealogy $G_T$ completely and uniquely.
When the genealogy is not clear from context, we write $y^{G}$ and $z^{G}$ to refer to the respective components of genealogy $G$.

Clearly, the topological structure of a tree imposes constraints on the permissible functions $y$ that describe the painting and node placement along it.
In particular, $y$ must respect the topological structure of $G_T$, such that if two lineages share a common ancestor at time $t$, both lineages must have been in the same deme at that time and the number of previous events along the lineage must agree.
In other words, if $a,b\in{c}\in{p}\in{\z_{t}}$, then $y_{t}(a)=y_{t}(b)$.
Let $\Yspace^z$ denote the space of all functions that respect the topological structure described by $z$.
Let $\Yspace^\z_t$ be the time-$t$ \emph{section} of $\Yspace^z$, \ie $\Yspace^\z_t=\displaystyle\bigcup_{y\in\Yspace^z}{\Set{y_t}}$.


\subsection{Jump types}\label{sec:jump_types}

We will show how a given population process naturally induces a process in the space of genealogies.
Because different kinds of jumps have different effects on the genealogy, we need a way of distinguishing them.
In particular, there are five distinct \emph{pure types} of jumps:
\begin{compactenum}[(a)]
\item \emph{Birth-type jumps} result in the branching of one or more new lineages, each from some existing lineages.
  Examples of birth-type events include transmission events, speciations, and actual births.
  For simplicity, we assume that all new lineages arising from a birth event share the same parent.
\item \emph{Death-type jumps} result in the extinction of one or more lineages.
  Examples include recovery from infection, death of a host, and species extinctions.
  We allow for the possibility that multiple lineages die simultaneously.
\item \emph{Migration-type jumps} result in the movement of a lineage from one deme to another.
  Spatial movements, changes in host age or behavior, and progression of an infection can all be represented as migration-type events.
  We permit multiple movements to occur simultaneously.
\item \emph{Sample-type jumps} result in the collection of a sample from a lineage but do not in themselves affect the inventory process.
  We allow for the possibility that multiple samples are collected in a single sample event.
  In this case, we assume that, in each sample-type event, each extant lineage is sampled at most once.
\item \emph{Neutral-type jumps} result in no change to any of the lineages.
\end{compactenum}
\Cref{fig:markov_state} depicts an example with all five of the pure types.
It is not necessary that a jump be of a pure type.
Compound jumps partake of more than one type.
For example, a sample/death-type event, in which a lineage is simultaneously sampled and removed, has been proposed \citep{Leventhal2014}, as have birth/death events in which one lineage reproduces at the same moment that another dies (\eg the Moran process \citep{Moran1958}).
The theory presented here places few restrictions on the complexity of the events that can occur.

%% Because different kinds of events may differ not only in the number of offspring they engender, but also in the number of parent lineages, and the distribution of offspring among parents and demes, there is implicitly a deterministic indicator function $Q_u$, for $u\in\Jumps$, (described below) that captures these properties.

\subsection{Genealogy process}

We now show how a given population process induces a stochastic process, $\Gen_t$, on the space of genealogies.
In the case of unstructured population processes (\ie those having a single deme), \citet{King2022} gave a construction.
Although we now treat a more general case, the ideas are much the same.
Readers wishing more detail on the construction here should consult the earlier paper.

At each event in the population process, one or more of the following changes happen to the genealogy, according to the type of the event:
\begin{compactenum}[(a)]
\item
  A birth-type event at time $t$ results in the creation of one new internal node, call it $b$.
  A tip node, $a$, of the appropriate deme is chosen with uniform probability from among those present and $b$ is inserted so that its ancestor is that of $a$, while $a$ takes $b$ as its ancestor.
  One new tip node, of the appropriate deme, is created for each of the children, all of which take $b$ as their immediate ancestor.
\item
  In a death-type event, one or more tip nodes of the appropriate demes are selected with uniform probability from among those present.
  These are deleted.
  Next, branch nodes without children are recursively removed.
  Sample nodes are never removed.
\item
  In a migration-type event, the appropriate number of migrating lineages are selected at random, with uniform probability according to deme, from among those present.
  For each selected lineage, one new branch node is inserted between the selected tip node and its ancestor.
  The color of the descendant branch changes accordingly.
\item
  At a sample-type event, the appropriate number of sampled lineages are selected at random from among the tip nodes, with uniform probability according to deme.
  One new sample node is introduced for each selected lineage:
  each is inserted between a selected tip nodes and its ancestor.
\item
  At a neutral-type event, no change is made to the genealogy.
\item
  Finally, events of compound type are accommodated by combining the foregoing rules.
\end{compactenum}
In each of these events, the new node or nodes that are introduced are situated at time $t$.

\begin{figure}
  \caption{
    Illustration of genealogy processes.
    \AAK{[Similar to that of \citet{King2022} but with multiple demes represented.]}
  }
\end{figure}

\subsubsection{Emergent lineages and production}

The lineages which descend from an inserted node are said to \emph{emerge} from the event.
Thus, after a birth-type event, the emerging lineages include all the new offspring as well as the parent.
Likewise, at pure migration- or sample-type events, each migrating or sampled lineage emerges from the event.
At pure death-type events, no lineages emerge.
In general, at an event of mark $u$, there are $r^u_i$ emergent lineages in deme $i$.
We require that $r^u_i$ be a constant, for each $u$ and $i$.
Since one is free to expand the set of jump-marks $\Jumps$ as needed, this is not an important restriction on the models that the theory can accommodate.
Thus there is a function $r:\Jumps\times\Demes\to\Zp$, such that $r^u_i$ lineages of deme $i$ emerge from each event of mark $u$.
We say $r^u=(r^u_i)_{i\in\Demes}$ is the \emph{production} of an event of mark $u$.
Note that the lineages that die as a result of an event do not count in the production but that all parent lineages, if they survive the event, are counted in the production.

\subsubsection{Conditional independence and exchangeability}

Application of these rules at each jump of $\Xr_t$ constructs a chain of genealogies $\Grh_k$.
In particular, at each jump-time $\Trh_k$, the genealogy $\Grh_{k-1}$ is modified according to the jump-mark $\Urh_k$ to yield $\Grh_k$.
We view $\Grh_k$ as the embedded chain of the continuous-time genealogy process $\Gen_t$.
It is very important to note that, conditional on $\Urh_k$, the number of parents and number of offspring in each deme is determined and the random choice of which lineages die, migrate, are sampled, or sire offspring is independent of the choices at any other times and independent of $\Xrh_k$ for all $k$.
Moreover, by construction, any lineage within a deme is as likely as any other lineage in that deme to be selected as a parent or for death, sampling, or migration.
We refer to this property as the \emph{exchangeability} of lineages within a deme.

\subsection{Pruned and obscured genealogies}

\subsubsection{Pruned genealogy}

Although the process just described yields a genealogy that relates all extant members of the population, and all samples, the data we ultimately wish to analyze will be based only on samples.
We therefore describe how the genealogy process is \emph{pruned} to yield the sample-only genealogy.
Given a genealogy $\Gen_t$, one obtains the \emph{pruned genealogy}, $\Pr_t=\prune(\Gen_t)$ by first dropping every tip node and then recursively dropping every childless internal node.
In a pruned genealogy only internal and sample nodes remain, and sample nodes are found at all of the leaves and possibly some of the interior nodes of the genealogy.
Observe that a pruned genealogy is a colored genealogy:
it retains information about where among the demes each of its lineages was through time.
Note also that a pruned genealogy $\Pr_t$ is characterized by functions $y^{\Pr_t}$ and $z^{\Pr_t}$ just as an un-pruned genealogy is.

\subsubsection{Lineage count and saturation}

In the following, we will find that we need to count the deme-specific numbers of lineages present in a given pruned genealogy at a given time.
Accordingly, suppose $\Pr_T=(y,z)$ is a pruned genealogy and suppose $t\in[0,T]$.
Let $\ell_i$ denote the number of lineages in deme $i$ at time $t$ and $\ell=(\ell_i)_{i\in\Demes}\in\Zp^{\Demes}$.
Clearly, $\ell$ depends only on the values, $(y_t,\z_t)$, of $y$ and $z$ at time $t$.
Therefore, we can define $\ell$ as a function such that, whenever $\Pr_T=(y,z)$ is a pruned genealogy, $\ell(y_t,\z_t)$ is the vector of deme-specific lineage counts at time $t$.
We refer to $\ell$ as the \emph{lineage-count} function.

We will also have occasion to refer to the deme-specific number of lineages emerging from a given event.
In particular, given a node time $t$ in a pruned genealogy $\Pr_T=(y,z)$, the number $s_i$ of lineages of deme $i$ emerging from all nodes with time $t$ is well defined and we can write $s=\left(s_i\right)_{i\in\Demes}$.
Like the lineage-count function, $s$ depends only on the local structure of $\Pr_T$.
However, $s$ depends not only on $(\y_t,\z_t)$, but also on $(\yt_t,\zt_t)$.
Thus, we can define the \emph{saturation} function such that, whenever $\Pr_T=(\y,\z)$ is a pruned genealogy, $s(\yt_t,\y_t,\zt_t,\z_t)$ is the integer vector of deme-specific numbers of emerging lineages at time $t$.

\begin{figure}
  \caption{
    Figure showing lineage count and saturation.
    \label{fig:ells}
  }
\end{figure}

\subsubsection{Obscured genealogy}

As we have seen, a pruned genealogy contains information about the full history of each sample lineage, including the times at which it entered or exited any deme, sired offspring, or was sampled.
The data we seek to analyze will typically lack much of this information.
Accordingly, we define the \emph{obscured genealogy} to be that obtained by discarding all information about demes and events not visible from the topology of the tree alone.
In particular, if $\Pr_T=(y,z)$ is a pruned genealogy, we write $\obs(\Pr_T)=z$ to denote the obscured genealogy.

\section{Results}

\subsection{Likelihood for pruned genealogies}

Our first result will be an expression for the likelihood of a given pruned genealogy $\P^*_T$ given the history $\Hr_T$ of the population process to time $T$.
Of course, not every history is compatible with $\P^*_T$.
In particular, $\Hr_T$ is only compatible with $\P^*_T$ if every node-time in $\P^*_T$ is an event time of $\Hr_T$, \ie only if $\event{\Hr}{T}\supseteq\event{P}{T}^*$.
Moreover, even if an event in $\Hr_T$ coincides with one of the node times of $\P^*_T$, it may be incompatible with the local structure of $\P^*_T$.
For example, if no sample is present in $\P^*_T$ at time $t$ and $\Hr_T$ contains an event of sample type at that time, then incompatibility arises.
As another example, if $\P^*_T$ has a node at time $t$ at which a lineage moves from deme $i$ to deme $i'$, but there is no $u\in\Jumps$ for which such a move is possible, then incompatibility arises again.
To keep track of this, let us introduce the indicator function $Q$ such that $Q_u(y,y',z,z')=1$ if and only if there is some pruned genealogy $\P_T$ and $t\in{[0,T]}$,
such that $\yt^{\P_T}_t=y$, $y^{\P_T}_t=y'$, $\zt^{\P_T}_t=z$, $z^{\P_T}_t=z'$,
and $\P_T$ is compatible with an event of type $u$ at time $t$;
$Q=0$ otherwise.

\begin{thm}\label{thm:pruned_lik}
  Let $\P^*_T=(y^*,z^*)$  be a given pruned genealogy and
  \begin{equation}\label{eq:phidef}
    \phi_u(x,y,y',z,z')\coloneq\BinRatio{n(x)}{\ell(y',z')}{r^{u}}{s(y,y',z,z')}\,Q_{u}(y,y',z,z')
  \end{equation}
  (cf.~\cref{sec:binomial_ratio}).
  Then
  \begin{equation*}
    \CondProb{\Pr_T=\P^*_T}{\Hr_T=\H_T}=\prod_{t\in\event{\H}{T}}{\phi_{\U_t}(\X_t,\yt^*_t,y^*_t,\zt^*_t,z^*_t)}
    \:\times\:\Indicator{\event{\H}{T}\supseteq{\event{\P}{T}^*}}.
  \end{equation*}
\end{thm}
\begin{proof}
  As we have already observed, if $\event{\Hr}{T}\nsupseteq\event{\P}{T}^*$, then $\CondProb{\Pr_T=\P^*_T}{\Hr_T}=0$.
  Similarly, if there is any event of $\Hr_T$ which is incompatible with $\P^*_T$, $\CondProb{\Pr_T=\P^*_T}{\Hr_T}=0$.
  Let us therefore suppose that neither of these conditions hold.
  Conditional on $\Hr_T=\H_T$, at each time $t\in\event{\H}{T}$, a jump of mark $\U_t$ occured, with a production of $r^{\U_t}=(r_i)_{i\in\Demes}$, resulting in a new deme-occupancy of $n(\X_t)=(n_i)_{i\in\Demes}$.
  In $\P^*_T$, at time $t$, there are $\ell_i=\ell_i(y^*_t,z^*_t)$ lineages in deme $i$, of which $s_i=s_i(\yt^*_t,y^*_t,\zt^*_t,z^*_t)$ are emergent.
  By assumption, at each genealogical event, lineages within a deme are exchangeable:
  each has an identical probability of being involved.
  This exchangeability implies that each lineage present in a deme at time $t$ was equally likely to have been one of the emergent lineages.
  In particular, at time $t$, the probability that $s_i$ of the $\ell_i$ deme-$i$ lineages were among the $r_i$ of $n_i$ lineages emergent in the inventory process is the same as the probability that, upon drawing $\ell_i$ balls without replacement from an urn containing $r_i$ red balls and $n_i-r_i$ black balls, exactly $s_i$ of the drawn balls are red, namely
  \begin{equation*}
    \frac{\binom{n_i-\ell_i}{r_i-s_i}\,\binom{\ell_i}{s_i}}{\binom{n_i}{r_i}}.
  \end{equation*}
  Because our lineages are labelled, each of the $\binom{\ell_i}{s_i}$ equally probable sets of $s_i$ lineages is distinct; just one of these is the one present in $\P^*_t$.
  Moreover, since, again conditional on $\Hr_T$, the identities of the lineages involved in a genealogical event are random and independent of the identities selected at all other events, we have established that
  \begin{equation*}
    \CondProb{\Pr_T=\P^*_T}{\Hr_T=\H_T}=\prod_{t\in\event{\H}{T}}{\BinRatio{n(\X_t)}{\ell(y^*_t,z^*_t)}{r^{\U_t}}{s(\yt^*_t,y^*_t,\zt^*_t,z^*_t)}}.
  \end{equation*}
  Returning to the possibility that $\H_T$ is incompatible with $\P^*_T$, since $\Prob{\Pr_T=\P^*_T}=0$ if either any $Q_u=0$ or any event of $\P^*_T$ is not an event of $\H_T$, we obtain the result.
\end{proof}

%% \begin{equation*}
%%   \phi_u(\xi,\eta,\eta')\colonequals\BinRatio{n(\xi)}{\ell(\eta')}{r^u}{s(\eta,\eta')}\,Q_u(\eta,\eta').
%% \end{equation*}

Next, we show how the likelihood of a pruned genealogies, unconditional on the history, can be computed.
For this, we use the filter equation technology developed in \cref{sec:filter_eqns}.
In particular, the following theorem follows immediately from \cref{prop:fullfilt}.

\begin{thm}
  Suppose that $\P^*_T=(y^*,z^*)$ is a pruned genealogy, for $T>0$.
  If $w=w(t,x)$ satisfies the initial condition $w(0,x)=p_0(x)$ and the filter equation
  \begin{equation}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}(t,x)=
      &\sum_u{\int{w(t,x')\,\alpha_u(t,x',x)\,\phi_u(x,\yt^*_t,y^*_t,\zt^*_t,z^*_t)\,\dd{x'}}}
      -\sum_u{\int{w(t,x)\,\alpha_u(t,x,x')\,\dd{x'}}}\\
      &\quad+\sum_{e\in\event{\P}{T}^*}{\delta(t,e)\,\left\{\sum_u{\int{w(t,x')\,\alpha_u(t,x',x)\,\phi_u(x,\yt^*_t,y^*_t,\zt^*_t,z^*_t)\,\dd{x'}}}-w(t,x)\right\}},
    \end{aligned}
  \end{equation}
  where $\phi$ is defined in \cref{eq:phidef},
  then the likelihood of $\P^*_T$ is
  \begin{equation*}
    \lik(\P^*_T)=\int{w(T,x)\,\dd{x}}.
  \end{equation*}
\end{thm}

\subsection{Likelihood for obscured genealogies}

Our next result concerns the likelihood of a given obscured genealogy conditional on the history.

\begin{thm}\label{thm:obsc_lik}
  Let $z^*=\obs(\P^*_T)$ be a given obscured genealogy.
  Let $\pi$ and $q$ be probability kernels, such that for all $u\in\Jumps$, $t\in\Rp$, $x,x'\in\Xspace$, $y,y'\in\Yspace^{z^*}_t$, $\pi_u(t,x,x',y,y')\ge{0}$, $\sum_{y'\in\Yspace_t^{z^*}}{\pi_u(t,x,x',y,y')}=1$, $q(x,y)\ge{0}$, and $\sum_{y\in\Yspace_0^{z^*}}{q(x,y)}=1$.
  Suppose moreover that $\pi_u(t,x,x',y,y')>0$ whenever $Q_u(y,y',\zt^*_t,z^*_t)=1$
  and that $q(x,y)>0$ whenever $\Prob{\Xr_0=x,y^{\Pr_T}_0=y}>0$.
  Then there is a Markov jump process $\yr_t$ on $\Yspace_{z^*}$ such that
  \begin{equation*}
    \CondProb{\zr_T=z^*}{\Hr_T=\H_T}=
    \Expect{\frac{1}{q(X_0,\yr_0)}\,\prod_{t\in\event{\H}{T}}{\frac{\phi_{\U_t}(\X_t,\yrt_t,\yr_t,\zt^*_t,\z^*_t)}{\pi_{\U_t}(t,\Xt_t,\X_t,\yrt_t,\yr_t)}}}
    \times\Indicator{\event{\H}{T}\supseteq\event{\z}{T}^*},
  \end{equation*}
  where $\phi$ is defined in \cref{eq:phidef} and
  the expectation is taken over the sample paths of $\yr_t$, $t\in[0,T]$.
\end{thm}
\begin{proof}
  First, observe that, since $\obs$ is a deterministic operator,
  \begin{equation}\label{eq:IS1}
    \CondProb{\zr_T=z^*}{\Hr_T}=\CondExpect{\Indicator{\obs(\Pr_T)=z^*}}{\Hr_T}.
  \end{equation}
  Our strategy will be to evaluate \cref{eq:IS1} using importance sampling:
  we will propose pruned genealogies compatible with $z^*_t$ as sample paths from a Markov process on $\Yspace_{z^*}$ and
  evaluate the the expectation in \cref{eq:IS1} by summing over these paths.
  Conditional on $\Hr_T$, the initial distribution $q$ and probability kernel $\pi$ generate a Markov chain, $\yrh_k$, on $\Yspace_{z^*}$ such that
  \begin{equation*}
    \begin{gathered}
      \Prob{\yrh_0}=q(\yrh_0),
      \qquad
      \CondProb{\yrh_k}{\yrh_{k-1},\Hr_T=\H_T}=\pi_{\Uh_k}(\Th_k,\Xh_{k-1},\Xh_{k},\yrh_{k-1},\yrh_k).
    \end{gathered}
  \end{equation*}
  The required Markov process $\yr_t$ is the unique \cadlag\ process with event times $\Trh_k$ and $\yrh_k$ as its embedded chain.
  This construction of $\yr_t$ obviously guarantees that $\event{\H}{t}\supseteq\event{\y}{}\supseteq\event{\z}{}^*$.

  Let us define $\Pr(\yr)=(\yr,z^*)$.
  Then, by construction, for every $\yr$, $\obs(\Pr(\yr))=z^*$ and, conversely, for every pruned genealogy $\Pr_T$ satisfying $\obs(\Pr_T)=z^*$, $y^{\Pr_T}$ is a sample path of this process having nonzero probability.
  We therefore have that
  \begin{equation*}
    \CondProb{\zr_T=z^*}{\Hr_T=\H_T}=
    \Expect{\frac{\CondProb{\Pr_T=P(\yr)}{\Hr_T=\H_T}}{\pi(\yr\vert\H_T)}}
  \end{equation*}
  the expectation being taken with respect to the random process $\yr_t$.
  Here, by definition,
  \begin{equation*}
    \pi(\yr\vert\H_T)=\prod_{t\in\event{\H}{T}}{\pi_{\U_{t}}(t,\Xt_{t},\X_{t},\yrt_{t},\yr_{t})}.
  \end{equation*}
  The result then follows from \cref{thm:pruned_lik}.
\end{proof}

The final result shows how to compute the likelihood of an obscured genealogy.
It is an immediate consequence of \cref{thm:obsc_lik,prop:fullfilt}.

\begin{thm}
  Let $z^*$, be a given obscured genealogy.
  Let $\pi$ and $\yr_t$ be as in \cref{thm:obsc_lik}.
  For $t\in\Rp$, $x,x'\in\Xspace$, $y,y'\in\Yspace^{z^*}_t$, $u\in\Jumps$, define
  \begin{equation*}
    \begin{gathered}
      \psi_u(t,x,x',y,y')=\frac{\phi_u(x',y,y',\zt^*_t,z^*_t)}{\pi_u(t,x,x',y,y')},\\[2ex]
      \beta_u(t,x,x',y,y')=\alpha_u(t,x,x')\,\pi_u(t,x,x',y,y'),
    \end{gathered}
  \end{equation*}
  where $\phi$ is defined in \cref{eq:phidef}.
  Suppose $w=w(t,x,y)$ satisfies the filter equation
  \begin{equation}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}(t,x,y)=
      &\sum_{uy'}{\int{w(t,x',y')\,\beta_u(t,x',x,y',y)\,\psi_u(t,x',x,y',y)\,\dd{x'}}}\\
      &-\sum_{uy'}{\int{w(t,x,y)\,\beta_u(t,x,x',y,y')\,\dd{x'}}}\\
      &+\sum_{e\in\event{\z}{}^*}{\delta(t,e)\,\left\{\sum_{uy'}{\int{w(t,x',y')\,\beta_u(t,x',x,y,y')\,\psi_u(t,x',x,y',y)\,\dd{x'}}}-w(t,x,y)\right\}},
    \end{aligned}
  \end{equation}
  with the initial condition $w(0,x,y)=p_0(x)\,q(x,y)$.
  Then the likelihood of $V^*_T$ is
  \begin{equation*}
    \lik(V^*_T)=\sum_y{\int{w(T,x,y)\,\dd{x}}}.
  \end{equation*}
\end{thm}

\section{Examples}

\subsection{SEIRS}

Jumps: $\Jumps=\Set{\lab{Inf},\lab{Prog},\lab{Recov},\lab{Wane},\lab{Birth},\lab{Death_S},\lab{Death_E},\lab{Death_I},\lab{Death_R},\lab{Sample}}$.

Demes: $\Demes=\Set{\lab{E},\lab{I}}$.

Jump rates:
\begin{itemize}
\item $\alpha_{\lab{Inf}}(t,x,x')=\beta(t)\,\frac{x^{\lab{S}}x^{\lab{I}}}{N(t)}\,\Indicator{x'=x+(-1,1,0,0)}$
\item $\alpha_{\lab{Prog}}(x,x')=\rho\,x^{\lab{E}}\,\Indicator{x'=x+(0,-1,1,0)}$
\item $\alpha_{\lab{Recov}}(x,x')=\gamma\,x^{\lab{I}}\,\Indicator{x'=x+(0,0,-1,1)}$
\item $\alpha_{\lab{Wane}}(x,x')=\upsilon\,x^{\lab{R}}\,\Indicator{x'=x+(1,0,0,-1)}$
\item $\alpha_{\lab{Sample}}(t,x,x')=\psi\,x^I\,\Indicator{x'=x}$
\item $\alpha_{\lab{Birth}}(t,x,x')=B(t)\,\Indicator{x'=x+(1,0,0,0)}$
\item $\alpha_{\lab{Death}_k}(x,x')=\mu\,x^k\,\Indicator{x'^j=x^j-\delta_{jk}}$, $k\in\Set{\lab{S},\lab{E},\lab{I},\lab{R}}$
\end{itemize}

%% Nonlinear filtering equation:
%% \begin{equation}
%%   \begin{aligned}
%%     \frac{\partial{w}}{\partial{t}}(t,x,y,z)=&B(t)\,\left[\sum_{y'z'}w\big(t,x-(1,0,0,0),y',z'\big)-w\big(t,x,y,z\big)\right]\\
%%     &\qquad+\sum_{k\in\{\lab{S},\lab{E},\lab{I},\lab{R}\}}\mu\,(x^k+1)\,w(t,x+\delta_{k})-\mu\,x^k\,w(t,x)\\
%%     &\qquad+\upsilon\,x^{\lab{R}}\,w(t,x-(1,0,0,-1))\\
%%   \end{aligned}
%% \end{equation}

\section{Discussion}


\bibliographystyle{preprint}
\bibliography{phylopomp}

\appendix
\setcounter{equation}{0}
\titleformat{\section}[hang]{\large\bfseries}{Appendix \periodafter\thesection}{2ex}{\periodafter}{}
\renewcommand{\theequation}{\thesection\arabic{equation}}

\section{Filter equations}
\label{sec:filter_eqns}

Explicit expressions for the quantities that arise in this paper are not always readily available.
Here, we develop tools for manipulating complex expressions that are otherwise cumbersome.

\begin{defn}
  Suppose $\Xr_t$ is a continuous-time Markov process with Kolmogorov forward equation (KFE)
  \begin{equation}\label{eq:kfe2}
    \frac{\partial{u}}{\partial{t}}
    =\int{u(t,x')\,\beta(t,x',x)\,\dd{x'}}
    -\int{u(t,x)\,\beta(t,x,x')\,\dd{x'}}.
  \end{equation}
  Suppose that $B(t,x,x')\ge{0}$ and $\lambda(t,x)$ are given, real-valued, measurable functions.
  Let $\mathcal{M}\subseteq\Rp$ be countable such that $\mathcal{M}\cap[0,t]$ is finite for all $t$.
  We say that the equation
  \begin{equation}\label{eq:filtereq}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}
      =&\int{w(t,x')\,\beta(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\beta(t,x,x')\,\dd{x'}}
      -\lambda(t,x)\,w(t,x)\\
      &\quad+\sum_{e\in\mathcal{M}}\delta(t,e)\,\left\{\int{w(t,x')\,\beta(t,x',x)\,B(t,x',x)\,\dd{x'}}-w(t,x)\right\}.
    \end{aligned}
  \end{equation}
  is the \emph{filter equation} with \emph{driver} $\Xr_t$ (or \emph{generated by} $\beta$), \emph{boost} $B$, \emph{decay} $\lambda$, and \emph{observed events} $\mathcal{M}$.
\end{defn}

\begin{remark}\label{rem:equivfilt}
  \Cref{eq:filtereq} is equivalent to
  \begin{equation*}
    \begin{gathered}
      \frac{\partial{w}}{\partial{t}}
      =\int{w(t,x')\,\beta(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\beta(t,x,x')\,\dd{x'}}
      -\lambda(t,x)\,w(t,x),\quad t\notin{\mathcal{M}},\\
      w(t,x)=\int{\leftlim{w}(t,x')\,\beta(t,x',x)\,B(t,x',x)\,\dd{x'}},\quad t\in{\mathcal{M}}.
    \end{gathered}
  \end{equation*}
  Here $\wt$ refers to the left-limit of $w$.
\end{remark}

\begin{remark}
  Trivially, a Kolmogorov forward equation is itself a filter equation with boost $1$, decay $0$, and no observed events ($\mathcal{M}=\emptyset$).
\end{remark}

Filter equations afford a convenient means of computing expectations and likelihoods for pure jump processes.
This is facilitated by the following fact.

\begin{lemma}\label{lemma:filtereq}
  The filter equation \eqref{eq:filtereq} with $\mathcal{M}=\emptyset$ and $B>0$ is satisfied by $w(t,x)=\int_0^{\infty}{v\,u(t,x,v)\,\dd{v}}$, where $u(t,x,v)$ satisfies the KFE
  \begin{equation}\label{eq:filterlemma}
    \begin{aligned}
      \frac{\partial{u}}{\partial{t}}=&\int{u(t,x',v')\,\beta(t,x',x)\,\delta(v,B(t,x',x)\,v')\,\dd{x'}\,\dd{v'}}\\
      &\qquad-\int{u(t,x,v)\,\beta(t,x,x')\,\delta(v',B(t,x,x')\,v)\,\dd{x'}\,\dd{v'}}
      +\tfrac{\partial}{\partial{v}}\left[\lambda(t,x)\,v\,u(t,x,v)\right],
    \end{aligned}
  \end{equation}
  on the space $\Xspace\times(0,\infty)$.
  Here, $\delta(v,v')$ is the familiar Dirac $\delta$.
\end{lemma}
\begin{proof}
  \ELI{[Conditions for differentiating under the integral in the proof?
      Need $[v^2 \lambda u(t,x,v)]_{v=0}^{\infty} = 0$.
      That's okay at $v=0$, and a minor constraint on $u$.]}
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}
      =&\int{v\,\frac{\partial{u}}{\partial{t}}(t,x,v)\,\dd{v}}\\
      =&\int{v\,u(t,x',v')\,\beta(t,x',x)\,\delta(v,B(t,x',x)v')\,\dd{v}\,\dd{x'}\,\dd{v'}}\\
      &\qquad-\int{v\,u(t,x,v)\,\beta(t,x,x')\,\delta(v',B(t,x,x')v)\,\dd{v}\,\dd{x'}\,\dd{v'}}\\
      &\qquad+\int{v\,\tfrac{\partial}{\partial{v}}\left[\lambda(t,x)\,v\,u(t,x,v)\right]\,\dd{v}}.\\
    \end{aligned}
  \end{equation*}
  Evaluating the first integral with respect to $v$, the second with respect to $v'$, and the third by parts, we obtain
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}
      =&\int{v'\,u(t,x',v')\,\beta(t,x',x)\,B(t,x',x)\,\dd{v'}\,\dd{x'}}
      -\int{v\,u(t,x,v)\,\beta(t,x,x')\,\dd{v}\,\dd{x'}}\\
      &\qquad-\lambda(t,x)\,\int{v\,u(t,x,v)\,\dd{v}},
    \end{aligned}
  \end{equation*}
  which is simplified to obtain \cref{eq:filtereq}.
\end{proof}

\Cref{eq:filterlemma} is recognizable as the KFE of a certain process $(\Xr_t,\Vr_t)$.
In particular, $\Xr_t$ is the driver with KFE \eqref{eq:kfe2}.
The $\Vr_t$ is \emph{directed} by $\Xr_t$ in the sense that $\Vr$ has jumps wherever $\Xr$ does:
when $\Xr$ jumps at time $t$ from $x$ to $x'$, $\Vr$ jumps by the multiplicative factor $B(t,x,x')>0$.
Between jumps, $\Vr_t$ decays deterministically and exponentially at rate $\lambda(t,x)$.
If we view $\Vr_t$ as a weight, then \cref{lemma:filtereq} tells us how the $\Vr_t$-weighted average of $\Xr_t$ evolves in time:
this average is simply $\int\!{w(t,x)\,\dd{x}}$.
This motivates the following result, which shows how filter equations allow one to integrate over random histories.

\begin{prop}\label{prop:zeroboost}
  Suppose $\Xr_t$ is a non-explosive pure jump process with KFE
  \begin{equation*}
    \begin{gathered}
      \frac{\partial{u}}{\partial{t}}(t,x)=\int{u(t,x')\,\alpha(t,x',x)\,\dd{x'}}
      -\int{u(t,x)\,\alpha(t,x,x')\,\dd{x'}},\qquad
      u(0,x)=p_0(x).
    \end{gathered}
  \end{equation*}
  Let $\Hr_t$ be its history process.
  Suppose $\Vr_t$ is an $\Rp$-valued random process such that
  \begin{equation}\label{eq:Vprod}
    \CondExpect{\Vr_t}{\Hr_t=\H_t}=\prod_{e\in\event{\H}{t}}{B(e,\Xt_e,\X_e)},
  \end{equation}
  for some given measurable function $B\ge{0}$.
  If $w$ satisfies the filter equation,
  \begin{equation*}
    \frac{\partial{w}}{\partial{t}}(t,x)
    =\int{w(t,x')\,\alpha(t,x',x)\,B(t,x',x)\,\dd{x'}}
    -\int{w(t,x)\,\alpha(t,x,x')\,\dd{x'}}\\
  \end{equation*}
  then $\Expect{\Vr_t}=\int{w(t,x)\,\dd{x}}$.
\end{prop}
\begin{proof}
  Define $Q(t,x,x')=\Indicator{B(t,x,x')>0}$ and apply \cref{lemma:filtereq} with
  \begin{equation*}
    \begin{gathered}
      \beta(t,x,x')=\alpha(t,x,x')\,Q(t,x,x'),
      \qquad
      \lambda=\int{\alpha(t,x,x')\,\left(1-Q(t,x,x')\right)\dd{x'}}.
    \end{gathered}
  \end{equation*}
  \ELI{[Lemma A1 has $B>0$, which only covers the cases when $Q=1$ and $\lambda=0$.
      But, $B\ge{0}$ seems enough in the proof of lemma A1, as long as one allows a one-sided Dirac $\delta$ at $0$.
      Maybe the Dirac $\delta$ should all be one-sided in order to integrate
      correctly on right-continuous functions? ]}
\end{proof}

%% An important special case is that of a deterministic driving process.
%% The following result is established by routine calculation.

%% \begin{prop}
%%   Suppose $X:[0,T]\to\Xspace$ is a deterministic, piecewise constant, \cadlag\ function and let $M=\event(X)$.
%%   Then the KFE for $X_t$ is \cref{eq:kfe2} with $\beta(t,x,x')=\sum_{e\in{M}}{\delta(t,e)\delta(x',X_e)}$.
%%   With this driver, the filter equation (\cref{eq:filtereq}) becomes
%%   \begin{equation*}
%%     \begin{gathered}
%%       \frac{\partial{w}}{\partial{t}}=
%%       -\lambda(t,x)\,w(t,x)\quad\text{for}\quad{t\in[0,T]\setminus{M}},\\
%%       w(t,x)=\delta(x,X_t)\,\int{\wt(t,x')\,B(t,x',X_t)\,\dd{x'}}\quad\text{for}\quad{t\in{M}}.
%%     \end{gathered}
%%   \end{equation*}
%% \end{prop}

The following result extends \cref{prop:zeroboost} to the situation where $\mathcal{M}\ne\emptyset$, \ie where the times of some of the events in the history are observed.

\begin{prop}\label{prop:fullfilt}
  Let $\Xr_t$, $\Hr_t$, $B$, and $\Vr_t$ be as in \cref{prop:zeroboost}.
  Suppose that $\mathcal{M}\subseteq\halfclosed{0,t}$ is a fixed, finite set of observed event times.
  Let $\CondExpect{\Vr_t}{\event{\Hr}{t}\supseteq\mathcal{M}}$ denote the expectation of $\Vr_t$ conditional on $\event{\Hr}{t}\supseteq\mathcal{M}$.
  If $w(t,x)$ satisfies $w(0,x)=p_0(x)$ and the filter equation
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}
      =&\int{w(t,x')\,\alpha(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\alpha(t,x,x')\,\dd{x'}}\\
      &\quad+\sum_{e\in\mathcal{M}}\delta(t,e)\,\left\{\int{w(t,x')\,\alpha(t,x',x)\,B(t,x',x)\,\dd{x'}}-w(t,x)\right\},
    \end{aligned}
  \end{equation*}
  then $\CondExpect{\Vr_t}{\event{\Hr}{t}\supseteq\mathcal{M}}=\int{w(t,x)\,\dd{x}}$.
  \ELI{[(a) line 2: density $\rightarrow$ expectation.
      (b) the term $\delta(t,e)\,w(t,x)$ seems irrelevant:
      the derivative at a finite set of points only matters when it is a delta function.]}
\end{prop}
\begin{proof}
  Suppose $s\in[0,t]$ and let
  \begin{equation*}
    w(s,x)=\Expect{\prod_{e\in\event{\Hr}{s}}{B\left(e,\Xrt_e,\Xr_e\right)}}.
  \end{equation*}
  That is, $w(s,x)$ is the expectation (over histories) of the partial products of \cref{eq:Vprod}.
  If $s\notin{\mathcal{M}}$, then for $\Delta>0$ sufficiently small,
  \begin{equation*}
    \begin{aligned}
      w(s,x)=&\left(1-\Delta\,\int{\alpha(s-\Delta,x,x')\,\dd{x'}}\right)\,w(s-\Delta,x)\\
      &\qquad+\int{\alpha(s-\Delta,x',x)\,B(s-\Delta,x',x)\,w(s-\Delta,x')\,\dd{x'}}+o(\Delta).
    \end{aligned}
  \end{equation*}
  Taking the limit as $\Delta\downarrow{0}$, we obtain
  \begin{equation*}
    \frac{\partial{w}}{\partial{s}}(s,x)
    =\int{w(s,x')\,\alpha(s,x',x)\,B(s,x',x)\,\dd{x'}}
    -\int{w(s,x)\,\alpha(s,x,x')\,\dd{x'}}.
  \end{equation*}
  If $s\in\mathcal{M}$, then
  \begin{equation*}
    w(s,x)=\int{\wt(s,x')\,\alpha(s,x',x)\,B(s,x',x)\,\dd{x'}}.
  \end{equation*}
  By \cref{rem:equivfilt}, the result follows.
  \ELI{[The argument shows that the filter equation satisfies a KFE, but the limit expression seems justified by a similar KFE.
      Is the point that one KFE implies the other?]}
\end{proof}

\ELI{[Maybe, Prop A3 could be a version of Prop A2 allowing for discrete transitions at a finite set of times;
    a combination of cts time and discrete time Markov chains.
    That is all we need to extend Prop A2 to observation times, which can be viewed just as an extended Markov process including the observation events.
    That is, to make a distinction between ``observation events'' that give the sequences used to construct the phylogeny, and ``observed events'' which are determined points on the phylogeny.
    The latter are a realization of a random process, and the former are defined to be fixed a priori.]}


%% Filter equations allow us to pass easily between equivalent representations of a process.
%% For example, an equivalent way of representing $\Xr_t$ is in terms of its embedded chain and event times.
%% Let $\Xrh_k$ be the embedded chain of $\Xr_t$ and let $\Trh_k$ be the point process of its event times.
%% It is elementary that
%% \fontsize{10pt}{12pt}\selectfont
%% \begin{equation*}
%%   \begin{gathered}
%%     \Prob{\Xrh_k=\Xh_k\;\Big\vert\;\Xrh_{k-1}=\Xh_{k-1},\Trh_k=\Th_k}=\frac{\alpha(\Th_k,\Xh_{k-1},\Xh_k)}{\int{\alpha(\Th_k,\Xh_{k-1},x')\dd{x'}}},\\
%%     \Prob{\Trh_k>\Trh_{k-1}+t\;\Big\vert\;\Xrh_{k-1}=\Xh_{k-1},\Trh_{k-1}=\Th_{k-1}}=\exp{\left(-\int_{0}^{t}{\int{\alpha(\Th_{k-1}+s,\Xh_{k-1},x')\,\dd{x'}}\,\dd{s}}\right)}.
%%   \end{gathered}
%% \end{equation*}
%% \normalfont
%% Fixing $\nu>0$ and making the definitions,
%% \begin{equation}\label[pluralequation]{eq:nudefs}
%%   \begin{gathered}
%%     A(t,x)=\int{\alpha(t,x,x')\,\dd{x'}}, \qquad
%%     \pi(t,x,x')=\frac{\alpha(t,x,x')}{A(t,x)},\\
%%     B(t,x,x')=\frac{A(t,x)}{\nu}, \qquad
%%     \lambda(t,x)=A(t,x)-\nu,
%%   \end{gathered}
%% \end{equation}
%% we can rewrite the KFE as
%% \begin{equation}\label{eq:poissdriver}
%%   \frac{\partial{w}}{\partial{t}}(t,x)=
%%   \int{w(t,x')\,\nu\,\pi(t,x',x)\,B(t,x,x')\,\dd{x'}}
%%   -\int{w(t,x)\,\nu\,\pi(t,x,x')\,\dd{x'}}
%%   -\lambda(t,x)\,w(t,x).
%% \end{equation}
%% Here, $\nu$ is the intensity of a time-homogenous Poisson process.
%% Note that $\pi$ is the probability kernel of the embedded chain $\Xrh$ and $A(t,x)$ is the intensity of the $\Trh_k$ process.
%% We recognize this equation as the filter equation with boost $B$, decay $\lambda$, and driver generated by $\nu\,\pi(t,x,x')$.
%% It corresponds to the following procedure for simulating $\Xr_t$:
%% \begin{compactenum}[(a)]
%% \item Simulate jump times according to the rate-$\nu$ Poisson process.
%% \item Simulate the embedded chain $\Xrh_k$ using the kernel $\pi$.
%% \item Weight the realization by the product of the $B$ factors.
%%   Note that this makes the appropriate importance-sampling correction.
%% \end{compactenum}

\end{document}
