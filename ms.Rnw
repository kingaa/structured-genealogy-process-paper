\documentclass[11pt,reqno,final]{amsart}
\usepackage[round,elide]{natbib}
\input{header}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,decorations,calc,math}

\title[Exact Phylodynamics]{Exact Phylodynamics via Structured Markov Genealogy Processes}
\author[King]{Aaron~A.~King}
\address{
  A.~A.~King,
  Department of Ecology \& Evolutionary Biology,
  Center for the Study of Complex Systems, and
  Department of Mathematics,
  University of Michigan,
  Ann Arbor, MI 48109 USA
}
\email{kingaa@umich.edu}
\urladdr{\href{https://kinglab.eeb.lsa.umich.edu/}{https://kinglab.eeb.lsa.umich.edu/}}
\author[Lin]{Qianying Lin}
\address{
  Q.-Y. Lin,
  Theoretical Biology and Biophysics,
  Los Alamos National Laboratory,
  Los Alamos, NM XXXXX USA
}
\author[Ionides]{Edward~L.~Ionides}
\address{
  E.~L.~Ionides,
  Department of Statistics
  University of Michigan,
  Ann Arbor, MI 48109 USA
}
\date{\today}

\hypersetup{pdftitle={Exact Phylodynamics via Structured Markov Genealogy Processes}}
\hypersetup{pdfauthor={A.A. King, Q.-Y. Lin, E.L. Ionides}}
\hypersetup{urlcolor=blue,citecolor=blue,linkcolor=blue,filecolor=blue}

<<prefix,include=FALSE,cache=FALSE,purl=FALSE>>=
prefix <- "smgp"
source("setup.R")
@
<<packages,include=FALSE,cache=FALSE>>=
library(tidyverse)
library(ggtree)
library(pomp)
library(cowplot)
library(phylopomp)
stopifnot(getRversion() >= "4.3")
stopifnot(packageVersion("pomp")>="5.1")
stopifnot(packageVersion("phylopomp")>="0.9.2")
theme_set(theme_bw(base_family="serif"))
set.seed(1159254136)
@

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

Problem of phylodynamics.
Factorization of problem into two subproblems.

Relation to previous work.
Existing methods \citep{Volz2009a,Stadler2010}.
Large-population, small sample-size approximations.

Extension of previous results \citep{King2022}.
Broader class of state-spaces.
Accommodating discrete structure.

Classes of Markov processes.
Utility and flexibility of Markov assumptions.

Population process induces Markov history and genealogy processes.
Using these, we derive equations for the likelihood of a genealogy conditional on the history.
We then integrate out the history to obtain nonlinear filtering equations, the solution of which yields the likelihood.
These readily lend themselves to a family of sequential Monte Carlo algorithms for computing the likelihood.
We demonstrate with several examples.

In the following, we show a Markov population process of the kind that is a staple in epidemiology induces a Markov process on the space of genealogies.
We then show how one can comput the likelihood of a given genealogy.

\section{Mathematical definitions}

\subsection{Population processes}

Motivating examples: compartmental models.
Wide variety of models.
Linear chain trick.
Migration, superspreading, competition between strains.

\begin{figure}
  \begin{center}
    \resizebox{0.9\linewidth}{!}{\input{figs/model_diagrams}}
  \end{center}
  \caption{
    Examples of compartmental models.
    Demes are shaded.
    \AAK{[Perhaps another one or two examples here?]}
    \AAK{[We could add dots to the deme compartments to signify individuals\dots.]}
    \label{fig:examples}
  }
\end{figure}

Another perspective on the Markov processes is to be had from its Markov state transition diagram (\cref{fig:markov_state}).

\begin{figure}
  \begin{center}
    \resizebox{0.9\linewidth}{!}{
      \begin{tikzpicture}[scale=1]
        \usetikzlibrary{shapes,arrows,positioning}
        \tikzstyle{coordinate}=[inner sep=0pt,outer sep=0pt]
        \tikzstyle{state}=[shape=ellipse, color=black, draw, font=\large, fill=white, thick, minimum size=5em]
        \tikzstyle{trans}=[color=black, thick, >=stealth]
        \node[state] (s0) at (0,0) {$x=(S,E,I,R)$};
        \node[state] (s1) at (165:7.8) {$x'=(S-1,E+1,I,R)$};
        \node[state] (s2) at (195:7.1) {$x'=(S,E-1,I+1,R+1)$};
        \node[state] (s3) at (295:4) {$x'=(S,E,I-1,R+1)$};
        \node[state] (s4) at (355:9) {$x'=(S+1,E,I,R-1)$};
        \node[state] (s5) at (18:6) {$x'=(S,E,I,R)$};
        \draw[trans,->] (s0) -- (s1) node[midway,above,sloped] {$\mathrm{Trans}$};
        \draw[trans,->] (s0) -- (s2) node[midway,above,sloped] {$\mathrm{Prog}$};
        \draw[trans,->] (s0) -- (s3) node[midway,above,sloped] {$\mathrm{Recov}$};
        \draw[trans,->] (s0) -- (s4) node[midway,above,sloped] {$\mathrm{Wane}$};
        \draw[trans,->] (s0) -- (s5) node[midway,above,sloped] {$\mathrm{Sample}$};
        \node[font=\Large] (U) at ($(s0)+(-6,-5)$) {$\Jumps=\{\mathrm{Trans},\mathrm{Prog},\mathrm{Recov},\mathrm{Wane},\mathrm{Sample}\}$};
      \end{tikzpicture}
    }
  \end{center}
  \caption{
    Markov state transition diagram for an SEIR model.
    The state is characterized by four numbers, $S$, $E$, $I$, and $R$.
    From a given state $x$, there are five possible kinds of events $x\mapsto{x'}$ as shown.
    From the point of view of the induced genealogy process,
    $\mathrm{Trans}$ (transmission) is of birth type,
    $\mathrm{Prog}$ (progression) is of migration type,
    and $\mathrm{Recov}$ (recovery) is of death type,
    while $\mathrm{Wane}$ (loss or waning of immunity) is of neutral type.
    Note that, in this formulation, when a $\mathrm{Sample}$ (sampling) event occurs, the state does not change.
    \label{fig:markov_state}
  }
\end{figure}

\paragraph{Mathematical notation}

Denote the underlying probability space by $(\Omega,\Borel,\mathrm{Prob})$.
Throughout the paper, we will adopt the convention that a bold-face symbol (\eg $\rdm{X}$), denotes a random element while the same symbol in regular face ($X$) denotes an element in its range.
We will assume that our population process is a time-inhomogeneous Markov jump process, $\X_t$, parameterized by time $t\in\Rp\colonequals\{t\in\mathbb{R}\;\vert\;t\ge{0}\}$) and taking values in some space $\Xspace$.
In earlier work \citep{King2022}, we limited ourselves to the case $\Xspace=\Z^d$, but here we assume only that $\Xspace$ is a complete metric space with a countable dense subset, \ie a Polish space.
The population process is completely specified by its initial-state distribution, $p_0$, and its transition rates $\alpha$.
In particular, we suppose that
\begin{equation}\label{eq:ic}
  \Prob{\X_0\in\mathcal{E}}=\int_{\mathcal{E}}{p_0(x)\,\dd{x}}
\end{equation}
for all measurable sets $\mathcal{E}\subseteq\Xspace$.
For any $t\in\Rp$, $x,x'\in\Xspace$, we think of the quantity $\alpha(t,x,x')$ as the instantaneous hazard of a jump from $x$ to $x'$.
More precisely, the transition rates have the following properties:
\begin{equation*}
  \begin{gathered}
    \alpha(t,x,x')\ge{0}, \qquad \int_{\Xspace}{\alpha(t,x,x')\,\dd{x'}}<\infty,\\
  \end{gathered}
\end{equation*}
for all $t\in\Rp$ and $x,x'\in\Xspace$.
Henceforth, we understand that integrals are taken over all of $\Xspace$ unless otherwise specified.
Let $\N_t$ be the number of jumps that $\X$ has taken by time $t$.
We assume that $\N_t$ is a simple counting process so that
\begin{equation*}
  \begin{gathered}
    \Prob{\N_{t+\Delta}=n+1\;\vert\;\N_{t}=n}=\Delta\,\int{\alpha(t,x,x')\,\dd{x'}}+o(\Delta),\\
    \Prob{\N_{t+\Delta}>n+1\;\vert\;\N_{t}=n}=o(\Delta),\\
    \Prob{\X_{t+\Delta}\in\mathcal{E}\;\vert\;\X_{t}=x, \N_{t+\Delta}-\N_{t}=1}=\frac{\int_{\mathcal{E}}{\alpha(t,x,x')\,\dd{x'}}}{\int{\alpha(t,x,x')\,\dd{x'}}}.
  \end{gathered}
\end{equation*}
We will further assume that $\X_t$ is non-explosive, \ie, that $\Prob{\N_t<\infty}=1$ for all $t$.
\AAK{[Is this equivalent to non-explosivity? Or merely an implication?]}

The above may be compactly summarized by stating that if $w(t,x)$ satisfies the Kolmogorov forward equation (KFE),
\begin{equation}\label{eq:kfe}
  \frac{\partial{w}}{\partial{t}}(t,x)=\int\!w(t,x')\,\alpha(t,x',x)\,\dd{x'}-\int\!w(t,x)\,\alpha(t,x,x')\,\dd{x'},
\end{equation}
and if, moreover, $w(0,x) = p_0(x)$,
then $\int_{\mathcal{E}}{w(t,x)\,\dd{x}}=\Prob{\X_t\in\mathcal{E}}$ for every measurable $\mathcal{E}\subseteq{\Xspace}$.
\Cref{eq:kfe} is sometimes called the \emph{master equation} for $\X_t$.

Without loss of generality, one can assume, as we do here, that the sample paths $t\mapsto\X_t(\omega)$ for $\omega\in\Omega$ are right-continuous with left limits (\ie \cadlag).
In fact, all of the processes we will describe in this paper will be taken to have \cadlag\ sample paths, and we will frequently need to refer to their left-limits.
Accordingly, if $\rdm{Z}_t$ is any \cadlag\ random process, we define
\begin{equation*}
  \leftlim{\rdm{Z}}_t\colonequals
  \begin{cases}
    \lim_{t'\uparrow{t}}\rdm{Z}_t, & t>0,\\
    \rdm{Z}_0, & t=0.\\
  \end{cases}
\end{equation*}
Note that $\leftlim{\rdm{Z}}_t$ is thus left-continuous with right limits.
Denote the set of event times of $\rdm{Z}$ by $\event{\rdm{Z}}{t}=\{e\in[0,t]\;\vert\;\leftlim{\rdm{Z}}_e\ne{\rdm{Z}_e}\}$.

\paragraph{Structured populations, demes}

In an \emph{unstructured} Markov population process, every lineage is exactly like every other.
\citet{King2022} showed how every such process induces an unstructured Markov genealogy process.
Here, our aim is to expand the theory considerably by allowing our population of lineages to have discrete structure.
In particular, we suppose that there are a countable set of subpopulations that differ in their vital rates, but within each of which, individual lineages are statistically identical.
We call these subpopulations \emph{demes}, and use the symbol $\Demes$ to denote an index set for them.

For any $i\in\Demes$, we let $n_i(\X_t)$ denote the number of lineages present in deme $i$ at time $t$, \ie the \emph{occupancy} of deme $i$.
Thus $n(\X_t)\in\Zp^{\Demes}$ is the vector of deme occupancies.

\paragraph{Jump marks}

In the following, it will be useful to break the jumps into distinct categories.
For this purpose, we let $\Jumps$ be a countable set of jump \emph{marks} such that
\begin{equation*}
  \alpha(t,x,x')=\sum_u{\alpha_u(t,x,x')}.
\end{equation*}
In \cref{fig:markov_state}, we use the marks to distinguish biologically distinct events.
Here and in the following, sums over $u$ are taken over the whole of $\Jumps$ unless otherwise indicated.

Let us define the \emph{jump mark} process, $\U_t$, to be the mark of the latest jump as of time $t$.
As usual, we take the sample paths, $t\mapsto{\U_t(\omega)}$ for $\omega\in\Omega$, to be \cadlag.
Observe that $(\X_t,\U_t)$ is a Markov process but that $\U_t$ is not.

\subsection{Examples}

\paragraph{SEIRS model}

\paragraph{SIIR model}

\paragraph{Linear birth-death model}

\paragraph{Moran model and the Kingman coalescent}

\subsection{The history, inventory, and genealogy processes}

\paragraph{History process}

Consider the Markov process $(\X_t,\U_t)$.
We define its \emph{history process}, $\Hist_t$, to be the restriction of the random function $t\mapsto(\X_t,\U_t)$ to the interval $[0,t]$.
Note that $\Hist_t$ is itself a Markov process.
%% Conditional on $\Hist_t$, both $\X_t$ and $\U_t$ are deterministic, as of course are the Markov chains of event times $\rdm{\hat{T}}_k$ and the embedded chains $\rdm{\hat{X}}_k=\X_{\rdm{\hat{T}}_k}$ and $\rdm{\hat{U}}_k=\U_{\rdm{\hat{T}}_k}$.
Conditional on $\Hist_t$, both $\X_t$ and $\U_t$ are deterministic as are the embedded chain, $(\X_k,\U_k)$, and the point process of event times $\rdm{\hat{T}}_k$.
(Recall that $\rdm{\hat{X}}_k=\rdm{X}_{\rdm{\hat{T}}_k}$.)
The non-explosiveness assumption implies that, for every $t$, $\rdm{K}_t\coloneq\big\vert\event{\Hist}{t}\cap[0,t]\big\vert<\infty$ a.s.
The probability measure, $\pi^H$, for $\Hist_t$ can be expressed in terms of these:
\begin{equation*}%\label{eq:Hdens}
  \begin{aligned}
    \pi^{H}(\dd{H_t})
    =&p_{0}(\hat{X}_{0})\,\dd{\hat{X}_0}\,
    \prod_{k=1}^{K_{t}}{\left(\alpha_{\hat{U}_k}\left(\hat{T}_k,\hat{X}_{k-1},\hat{X}_{k}\right)\,\dd{\hat{X}_k}\,\dd{\hat{T}_k}\right)}\\
    &\qquad\times\exp{\left(-\sum_{k=1}^{K_t}{\int_{\hat{T}_{k-1}}^{\hat{T}_k}{\sum_{u}{\int{\alpha_{u}(t',\hat{X}_{k-1},x')\,\dd{x'}}}}\,\dd{t'}}\right)}.
  \end{aligned}
\end{equation*}

\paragraph{Inventories}

Our goal in this paper is to probabilistically characterize how the genealogical relationships among lineages evolve through time.
Accordingly, we develop some notation for this purpose.
To begin with, we assign to each lineage a unique number $j\in\Zp$.
This can be done in any fashion, so long as no two lineages ever receive the same number.
For example, when a new lineage arises, we can assign it the smallest integer that has not yet been assigned.
We will define the \emph{inventory process}, $\Inv_t$, so that, for every lineage $j\in\Zp$, $\Inv_t(j)$ is the deme in which $j$ is found.
However, when $t$ is before the birth or after the death of lineage $j$, then clearly $\Inv_t(j)\notin\Demes$.
We say in this case that lineage $j$ is in the \emph{underdeme}, which we denote using the symbol $\eth$, so that we can write $\Inv_t(j)=\eth$.
We define $\Demesplus\colonequals\Demes\cup\{\eth\}$ so that $\Inv:\Rp\times\Zp\to\Demesplus$.

The birth and death times of lineage $j$ are therefore
\begin{equation*}
  \begin{gathered}
    t^b_j=\min\{t|\Inv_t(j)\ne{\eth}\}\qquad
    \text{and}\qquad
    t^d_j=\sup\{t|\Inv_t(j)\ne{\eth}\},
  \end{gathered}
\end{equation*}
respectively.
Observe that $n_i(\X_t)=\left\vert\left\{j\;\vert\;\Inv_t(j)=i\right\}\right\vert$ for all $t\in\Rp$ and $i\in\Demes$.
Note also that $n$ does not count the inhabitants of the underdeme.

\paragraph{Jump types}

Different kinds of events that occur for the population process can have different kinds of effects on the inventory process, and indeed not every jump affects $\Inv_t$ at all.
From the point of view of the inventory process, there are five distinct \emph{pure types} of jumps, which we enumerate here.
\begin{enumerate}[(a)]
\item Birth-type events result in the branching of one or more new lineages, each from some existing lineage.
  If $j$ is one of the new lineages, we use the expression $\Anc(j)$ to refer to its ancestor.
  Examples of birth-type events include transmission events, speciations, and actual births.
  It is not assumed that all new lineages arising from a birth event share the same ancestor.
\item Death-type events result in the extinction of one or more lineages.
  Examples include recovery, death of a host, and species extinctions.
\item Migration-type events result in the movement of a lineage from one deme to another.
  Spatial movements, changes in host age or behavior, and progression of an infection can all be represented as migration-type events.
\item Sample-type events result in the collection of a sample from a lineage but do not in themselves affect the inventory process.
\item Neutral-type events result in no change to any of the lineages.
\end{enumerate}
\Cref{fig:markov_state} depicts an example with all five of the pure types.
It is not necessary that a jump fall into just one of these types.
It is allowable, for instance to have compound jumps that fall into more than one category.
For example, sample/death-type events, in which a lineage is simultaneously sampled and removed, have been used, as have birth/death events in which one lineage reproduces at the same moment that another dies.
The theory presented here places no restrictions on the complexity of the events that can occur.

However, we do impose the restriction that the \emph{production}, \ie the deme-specific number of lineages emerging from the event, be constant for all jumps of a given mark.
To be precise, the \emph{production} is defined to be a function $r:\Jumps\times\Demes\to\Zp$, such that $r^u_i$ lineages of deme $i$ emerge from each event of mark $u$.
We write $r^u=(r^u_i)_{i\in\Demes}$.
Note that we lineages that die as a result of an event do not count in the production.
Also, it is important to note that the parent lineage or lineages, if they survive the event, are always counted in the production.

Because different kinds of events may differ not only in the number of offspring they engender, but also in the number of parent lineages, and the distribution of offspring among parents and demes, there is implicitly a deterministic indicator function $Q_u$, for $u\in\Jumps$, (described below) that captures these properties.

\paragraph{Inventory process}

The structure of the state space for the inventory process, $\Inv_t$, has already been described.
It remains to define its stochastic dynamics.
The $\Inv_t$ process is driven by the population process $\X_t$ in that jumps in $\Inv_t$ do not occur except when jumps in $\X_t$ occur:
$\leftlim{\Inv}_t\ne\Inv_t$ implies $\leftlim{\N}_t\ne\N_t$.

At jumps of birth type and mark $u$, the appropriate number of random parents are selected from the appropriate deme(s) and each one sires the appropriate number of offspring in each deme.
At jumps of death type, the appropriate number of lineages are selected from the appropriate demes and moved to the underdeme.
At jumps of migration type, randomly selected lineages are moved between demes as appropriate.
At sample-type and neutral-type jumps, no change occurs.
Jumps of compound type are handled in the obvious way.

Crucially, the assumption that the population process $\X_t$ is Markov empowers us to assume that the individual lineages within each deme are exchangeable with respect to the inventory process.
Since the law governing $\X_t$ is independent of the identities of the individual lineages, and since the individual lineages are exchangeable by assumption, it follows that, conditional on $\Hist_t$, the jumps of the inventory are independent of one another.

\paragraph{Genealogies}

\citet{King2022}, showed how an unstructured population process induces a process on the space of genealogies.
Although we now treat a more general case, the construction is much the same, so we abbreviate the presentation.
Readers wishing for more detail should consult the earlier paper.

For our purposes, a \emph{genealogy} is a labeled, time-calibrated tree.
Its edges represent relationships of ancestry and descent among its nodes.
There are three distinct kinds of nodes:
\begin{inparaenum}[(i)]
\item \emph{tip nodes}, which represent labeled extant lineages;
\item \emph{internal nodes}, which represent ancestral events;
\item \emph{sample nodes}, which represent labeled samples.
\end{inparaenum}
More formally, we can take a genealogy, $G$, to be a finite sequence of internal nodes, together with a time.
Given $G$, the time is denoted $T(G)$; this is the time corresponding to the extant lineages.
The number of nodes is $K(G)$.
We order the nodes temporally, and denote the $k$-th node $\Node{p}_k(G)$ and we write $\Node{p}\in{G}$ if $\Node{p}$ is one of the nodes of $G$.
Each $\Node{p}\in\Gen$ has a creation-time, $T(\Node{p})$, a parent, $\Anc(\Node{p})$, and a deme.
Moreover, $\event{G}{t}$ is just the sequence of node times:
$\event{G}{t}=\left(T(\Node{p}_k(G))\right)_{k=1}^{K(G)}$.
%% The nodes are ordered so that $k<k'$ implies that $T(\Node{p}_k(\Gen))\le{T(\Node{p}_{k'}(\Gen))}$.
Root nodes are distinguished by being their own parents:
$\Node{p}$ is a root if and only if $\Anc(\Node{p})=\Node{p}$.
Every node also has one or more descendant nodes called \emph{children}.

\paragraph{Genealogy processes}

The population and inventory processes together induce a stochastic process, $\Gen_t$, on the space of genealogies.
In particular, at each event in the population process, one or more of the following changes happen to the genealogy, according to the type of the event:
\begin{enumerate}[(a)]
\item A birth-type event at time $t$ results in the creation of one new internal node for each parent lineage.
  In particular, if $j$ is one of the parent lineages, and $\Node{p}$ is the new node, then $T(\Node{p})=t$, $\Anc(\Node{p})$ is the node that was parent to $j$ prior to the event.
  The children of $\Node{p}$ include one new tip node for each new lineage sired by $j$, as well as $j$ itself.
\item In a death-type event, all the lineages $j$ that die are removed.
  Internal nodes without children are then recursively removed.
  Sample nodes are never removed.
\item In a migration-type event, one node is added for each migrating lineage;
  each one takes the migrating lineage as child.
  The ancestor of the new node is that which was parent to the migrating lineage prior to the event.
  The deme of the lineage changes accordingly.
\item At a sample-type event, one new sample node is introduced for each sampled lineage.
  Each one takes the sampled lineage as child.
  The ancestor of the sample node is that which was parent to the sampled lineage before the vent.
\item At a neutral-type event, no change is made to the genealogy.
\item Finally, events of compound type are accommodated by combining the foregoing rules.
\end{enumerate}
When an event results in the addition of one or more new nodes to a genealogy, the lineages which are children of that node are said to \emph{emerge} from the event.
Thus, after a birth-type event, the emerging lineages include all the new offspring as well as their parents.
Likewise, at pure migration- or sample-type events, each migrating or sampled lineage emerges from the event.
At pure death-type events, no lineages emerge.
In general, at an event of mark $u$, there are $r^u_i$ emergent lineages in deme $i$.

\begin{figure}
  \caption{
    Illustration of genealogy processes.
    \AAK{[Similar to that of \citet{King2022} but with multiple demes represented.]}
  }
\end{figure}

\subsection{The pruned and obscured genealogies}

Although the process just described yields a genealogy that relates all extant members of the population, and all samples, the data we ultimately wish to analyze will contain only the samples.
We therefore describe how the genealogy process is \emph{pruned} to yield the sample-only genealogy.
Given a genealogy $\Gen_t$, one obtains the \emph{pruned genealogy}, $\Prune_t=\prune(\Gen_t)$ by first dropping every tip node and then recursively dropping every internal node without children.
In a pruned genealogy only internal and sample nodes remain, and sample nodes are found at all of the leaves and some of the interior nodes of the genealogy.
Observe that the pruned genealogy retains information not only about how much ancestry is shared by any pair of sample lineages, but also about where among the demes each lineage was through time.
Note also that $T(\Prune_t)=T(\Gen_t)=t$.

\paragraph{Alternative representation of a pruned genealogy}

Consider the finite set of all samples represented in pruned genealogy $P$.
Beginning with 1, assign natural numbers to these in such a way that if sample $j$ is ancestral to sample $j'$, then $j<j'$.
For example, we can order the samples by their times, resolving any ties arbitrarily.
Let $\Lin(P)$ denote the set of lineage numbers.
Using this ordering, we can uniquely associate each point on a genealogical tree with the least of those lineages that descend from that point.
In particular, any lineage $j\in\Lin(P)$, corresponding to a sample taken at time $t^s_j$, can be traced backward from node to node until either it coalesces with some lesser (\ie older) lineage at some time $t^o_j>0$ or a root is reached (in which case, we define $t^o_j=0$).
Each node encountered along the way represents a genealogical event from which $j$ emerges.
Moreover, at each time $t\in\halfopen{t^o_j,t^s_j}$, lineage $j$ is in precisely one of the demes $\Demes$.
However, for $t\notin\halfopen{t^o_j,t^s_j}$, lineage $j$ does not exist.
To express this, we again say that lineage $j$ is in the underdeme, $\eth$.

It will be useful to distill the elements that characterize a pruned genealogy.
Accordingly, given a pruned genealogy, $P$, we make the following definitions.
\begin{compactenum}[(a)]
\item Let $\ct^{P}:\Zp\times\Rp\to\Zp$ be such that, for $j\in\Zp$, $\ct^{P}_j(t)$ is the counting process which increases by 1 at each event along lineage $j$.
\item Let $\anc^{P}:\Zp\times\Rp\to\Zp$ be such that $\anc^{P}_j(t)$ indicates the unique lineage, ancestral of lineage $j$, and alive at time $t$.
  In particular, we posit that $\anc^{P}_j(t)=j$ for $t\in\halfopen{t^o_j,t^s_j}$ and $\anc^{P}_j(t)=0$ for $t>t^s_j$, so that the function is well defined for all $j$ and $t$.
  %%  Alternatively, one can define $\anc$ using the original construction of Kingman's coalescent \citep{Kingman1982a}.
\item Let $\deme^{P}:\Zp\times\Rp\to\Demesplus$ be such that, for $j\in\Zp$, $\deme^{P}_j(t)$ indicates in which deme lineage $j$ lies at time $t$.
  In particular $\deme^{P}_j(t)=\eth$ if $t\notin\halfopen{t^o_j,t^s_j}$.
\item Let $\Yspace=(\Zp\times\Zp\times\Demesplus)^{\Zp}$ and define $Y^{P}:\Rp\to\Yspace$ such that, for $t\in\Rp$,
  $Y^{P}(t)=\left(\ct^{P}(t),\anc^{P}(t),\deme^{P}(t)\right)$.
\end{compactenum}
For $j>|\Lin(P)|$, we adopt the convention that $t^o_j=t^s_j=\infty$, so that $\ct^P_j(t)=0$, $\anc^{P}_j(t)=0$, and $\deme^P_j(t)=\eth$ for all $t\in\Rp$.
It is easy to see that $Y^{P}$ is well defined, piecewise constant, and \cadlag, and that the map $P\mapsto{Y^{P}}$ is one-to-one.
However, not every piecewise constant, \cadlag\ map $y:\Rp\to\Yspace$ defines a pruned genealogy.
If $y$ does define a pruned genealogy, we denote that pruned genealogy by $P(y)$.
Given $y\in\Yspace$, we will use the notation $\ct(y)$, $\anc(y)$, and $\deme(y)$ to refer to the coordinates of $y$.

Since $\Gen_t$ is a stochastic process, both $\Prune_t\colonequals\prune(G_t)$ and $Y^{\Prune_t}$ are stochastic processes as well.
In fact, the latter two are Markovian, since each contains within itself all of its past history.
The process $\Gen_t$ is not Markovian, though $(\X_t,\Gen_t)$ is.

To visualize $\Prune_t$, one can make a correspondence between demes and colors.
Then a pruned genealogy is visualized as a tree with colored branches.
Knowing the function $\deme^{\Prune_t}$ is equivalent to knowing the coloring, while $\ct^{\Prune_t}$ determines the locations of events in the genealogy and $\anc^{\Prune_t}$ determines the topology.
Note in particular that, if $y=Y^{\Prune_t}$, then $y_j(t')\ne\yt_j(t')$ if and only if $t'$ is the time of an event from which lineage $j$ emerges.
Note also that, there can be events on a pruned genealogy where the color does not change.
That is, where $\ct_j$ increments, but neither $\anc_j$ nor $\deme_j$ change.

\paragraph{Lineage count and saturation}

In the following, we will find that we need to count the deme-specific numbers of lineages present at a given time.
Accordingly, for any $\eta\in\Yspace$, and $i\in\Demes$, let us define
\begin{equation*}
  \ell_i(\eta)\colonequals{\left\vert\left\{j\in\Zp\;\vert\;\deme(\eta_j)=i\right\}\right\vert}\in\Zp, \qquad \ell(\eta)\colonequals(\ell_i(\eta))_{i\in\Demes}\in{\Zp^\Demes}.
\end{equation*}
Note that lineages $j$ for which $\deme(\eta_j)=\eth$ are not counted.
With this definition, it follows that, for any pruned genealogy $P$, $\ell_i(Y^{P}(t))$ is the number of lineages in deme $i$ at time $t$ and
$\ell(Y^{P}(t))\in\Zp^\Demes$ is the non-negative integer vector telling how many lineages lie within each of the demes at time $t$.

We will also have occasion to refer to the deme-specific number of lineages emerging from a given event.
Therefore, for $y,y'\in\Yspace$, and $i\in\Demes$, let us define
\begin{equation*}
  s_i(y,y')\colonequals{\left\vert\left\{j\in{\Zp}\;\vert\;\deme(y_j)=i\ \ \&\ \ \ct(y'_j)\ne\ct(y_j)\right\}\right\vert}, \qquad s(y,y')\colonequals(s_i(y,y'))_{i\in\Demes}\in{\Zp^\Demes}.
\end{equation*}
With this definition, for any $P$, if $y=Y^P$, then $s_i(\yt(t),y(t))$ is the number of lineages in deme $i$ that emerge from an event at time $t$ and $s(\yt(t),y(t))$ is the non-negative integer vector telling how many lineages in each deme emerge at time $t$.
In particular, $s(y,y)=0$ so that if $e\notin\event{P}{t}$, then $s(\yt(e),y(e))=0$.
We refer to $s\left(\yt(t),y(t)\right)$ as the \emph{saturation} at time $t$.

\paragraph{Obscured genealogy}

As we have just seen, a pruned genealogy contains information about the full history of each sample lineage, including the times at which it entered or exited any deme, sired offspring, or was sampled.
The data we seek to analyze will typically lack much of this information.
Accordingly, we define the \emph{obscured genealogy} to be that obtained by discarding all information about demes and events not visible from the topology of the tree alone.
In particular, if $\Prune$ is a pruned genealogy and $\Vis=\obs(\Prune)$ is the corresponding obscured genealogy,
$\Vis$ is uniquely determined by the function $t\mapsto\anc(Y^{\Prune}(t))$.

\paragraph{Binomial ratio}

For $n,r,\ell,s\in{\Zp^\Demes}$, define the \emph{binomial ratio}
\begin{equation*}
  \BinRatio{n}{\ell}{r}{s}\colonequals
  \begin{cases}
    \frac{\displaystyle\prod_{i\in\Demes}{\binom{n_i-\ell_i}{r_i-s_i}}}%
         {\displaystyle\prod_{i\in\Demes}{\binom{n_i}{r_i}}},
         & \forall i\ n_i\ge{\{\ell_i,r_i\}}\ge{s_i}\ge{0},\\
         0, & \text{otherwise}.
  \end{cases}
\end{equation*}
Observe that $\BinRatio{n}{\ell}{r}{s}\in{[0,1]}$.
Moreover, in consequence of the Chu-Vandermonde identity, we have
\begin{equation*}
  \sum_{s\in\Zp^{\Demes}}\BinRatio{n}{\ell}{r}{s}\binom{\ell}{s}=1,
\end{equation*}
whenever $n_i\ge{\{\ell_i,r_i\}}\ge{0}$ for all $i$.

\section{Results}

\paragraph{Likelihood conditional on history}

Our first result will be an expression for the likelihood of a given pruned genealogy $P^*_t$ given the history $\Hist_t$ of the population process to time $t$.
Of course, not every pruned genealogy is compatible with $\Hist_t$.
In particular, $P^*_t$ is only compatible with $\Hist_t$ if every event in $P^*_t$ coincides with an event in $\Hist_t$.
That is, only if $\event{P^*}{t}\subseteq{\event{\Hist}{t}}$.
Moreover, certain events in $P^*_t$ may be incompatible with all possible histories.
For example, if $P^*_t$ has an event in which a lineage moves from deme $i$ to deme $i'$ but there are no $u\in\Jumps$ for which this is possible, then $P^*_t$ is incompatible with the population process itself.
Let us define the function $Q:\Jumps\times\Yspace\times\Yspace\to\{0,1\}$ so that $Q_u(y,y')=1$ if and only if a change $y\to{y'}$ in a pruned genealogy is compatible with an event of mark $u$ at that time.
Recall that $n:\Xspace\to\Zp^{\Demes}$ denotes the deme-occupancy function, $r^u$, the production of a jump of mark $u$, $\ell$, the lineage-count function, and $s$, the saturation.

\begin{thm}\label{thm:pruned_lik}
  Let $P^*_t$ be a given pruned genealogy and $y^*=Y^{P^*_t}$.
  Then
  \begin{equation*}
    \begin{aligned}
      \Prob{P^*_t\vert\Hist_t}=&\prod_{e\in\event{\Hist}{t}}{\BinRatio{n(X_e)}{\ell(y^*(e))}{r^{U_e}}{s(\yt^*(e),y^*(e))}\,Q_{U_e}(\yt^*(e),y^*(e))}\\
      &\qquad\times\Indicator{\event{P^*}{t}\subseteq{\event{\Hist}{t}}}.
    \end{aligned}
  \end{equation*}
\end{thm}
\begin{proof}
  As we have already observed, if $\event{P^*}{t}\nsubseteq\event{\Hist}{t}$, then $\Prob{P^*_t\vert\Hist_t}=0$.
  Similarly, if there is any event of $P^*_t$ which is incompatible with the population process, $\Prob{P^*_t\vert\Hist_t}=0$.
  Let us therefore suppose that neither of these conditions hold.
  Recall that, conditional on $\Hist_t$, each jump of the inventory process is independent of the others.
  Moreover, at each event $e\in\event{\Hist}{t}$, a jump of mark $U_e$ occured, with a production of $r^{U_e}=(r_i)_{i\in\Demes}$, resulting in a new deme-occupancy of $n(X_e)=(n_i)_{i\in\Demes}$.
  In $P^*_t$, at time $e$, there are $\ell_i=\ell_i(y^*(t))$ lineages in deme $i$, of which $s_i=s_i(\yt^*(t),y^*(t))$ are emergent.
  The exchangeability of lineages within demes implies that each lineage present in a deme at time $e$ was equally likely to have been one of the emergent lineages.
  In particular, at time $e$, the probability that $s_i$ of the $\ell_i$ deme-$i$ lineages were among the $r_i$ of $n_i$ lineages emergent in the inventory process is the same as the probability that, upon drawing $\ell_i$ balls without replacement from an urn containing $r_i$ black balls and $n_i-r_i$ white balls, exactly $s_i$ of the drawn balls are black, namely
  \begin{equation*}
    \frac{\binom{n_i-\ell_i}{r_i-s_i}\,\binom{\ell_i}{s_i}}{\binom{n_i}{r_i}}.
  \end{equation*}
  Because our lineages are labelled, each of the $\binom{\ell_i}{s_i}$ equally probable sets of $s_i$ lineages is distinct; just one of these is the one present in $P^*_t$.
  Moreover, since, again conditional on $\Hist_t$, the inventory process within each deme is independent of the others, we have established that
  \begin{equation*}
    \Prob{P^*_t\vert\Hist_t}=\prod_{e\in\event{\Hist}{t}}{\BinRatio{n(X_e)}{\ell(y^*(e))}{r^{U_e}}{s(\yt^*(e),y^*(e))}}.
  \end{equation*}
  Returning to the possibility that $P^*_t$ is incompatible with $\Hist_t$, since $\Prob{P^*_t}=0$ if either any $Q_u=0$ or any event of $P^*_t$ is not an event of $\Hist_t$, we obtain the result.
\end{proof}

%% \begin{equation*}
%%   \phi_u(\xi,\eta,\eta')\colonequals\BinRatio{n(\xi)}{\ell(\eta')}{r^u}{s(\eta,\eta')}\,Q_u(\eta,\eta').
%% \end{equation*}

Our second result concerns the likelihood of a given obscured genealogy conditional on the history.

\begin{thm}\label{thm:obsc_lik}
  Let $V^*_t$ be a given obscured genealogy.
  Let $\pi:\Jumps\times\Rp\times\Xspace^2\times\Yspace^2\to\Rp$ be a probability kernel, \ie $\pi_u(t,x,x',y,y')\ge{0}$ and $\sum_{y'}{\pi_u(t,x,x',y,y')}=1$.
  Suppose moreover that $\pi_u(t,x,x',y,y')>0$ if and only if $Q_u(y,y')=1$.
  Let $\rdm{y}_t$ be the Markov jump process on $\Yspace$ generated by the kernel $\pi$, with $\event{\rdm{y}}{t}=\event{\Hist}{t}$.
  Then there is a measurable function $\kappa_{V^*_t}:[0,t]\times\Yspace^2\to\{0,1\}$ such that
  \begin{equation*}
    \begin{aligned}
      \Prob{V^*_t\vert\Hist_t}=&\Expect{\left.\prod_{e\in\event{\Hist}{t}}{\BinRatio{n(X_e)}{\ell(\rdm{y}_e)}{r^{U_e}}{s(\leftlim{\rdm{y}}_e,\rdm{y}_e)}\,\frac{\scriptsize\kappa_{V^*_t}(e,\leftlim{\rdm{y}}_e,\rdm{y}_e)}{\scriptsize\pi_{U_e}(e,\leftlim{X}_e,X_e,\leftlim{\rdm{y}}_e,\rdm{y}_e)}}\;\right\vert\;\Hist_t}\\
      &\qquad\times\Indicator{\event{V^*}{t}\subseteq\event{\Hist}{t}},
    \end{aligned}
  \end{equation*}
  the expectation being taken over $\rdm{y}$.
\end{thm}
\begin{proof}
  First, observe that, since $\obs$ is a deterministic operator,
  \begin{equation}\label{eq:IS1}
    \Prob{V^*_t\vert\Hist_t}=\int{\Indicator{\obs(P_t)=V^*_t}\,\Prob{P_t\vert\Hist_t}\,\dd{P_t}},
  \end{equation}
  the integral being taken over all pruned genealogies.
  Our strategy will be to evaluate \cref{eq:IS1} using importance sampling:
  we will propose pruned genealogies compatible with $V^*_t$ as sample paths from a Markov process on $\Yspace$ and
  treat the integral in \cref{eq:IS1} as an expectation over these paths.
  Conditional on $\Hist_t$, the probability kernel $\pi$ generates a Markov chain, $\rdm{\hat{y}}_k$, on $\Yspace$ such that
  \begin{equation*}
    \Prob{\rdm{\hat{y}}_k=\hat{y}_k\vert\rdm{\hat{y}}_{k-1}=\hat{y}_{k-1}}=\pi_{U_{t_k}}(t_k,\leftlim{X}_{t_k},X_{t_k},\hat{y}_{k-1},\hat{y}_k).
  \end{equation*}
  Now let $\event{\Hist}{t}=\{t_1,t_2,\dots,t_K\}$.
  $\rdm{\hat{y}}_k$ is the embedded chain of a unique, piecewise-constant, \cadlag\ process $\rdm{y}_t$, with jump times at $t_k$.
  This construction of $\rdm{y}_t$ obviously guarantees that $\event{P(\rdm{y})}{t}\subseteq\event{\Hist}{t}$.
  Moreover, the conditions on $\pi$ guarantee that, for every pruned genealogy $P_t$, $Y^{P_t}$ is a sample path of this process.
  In particular, they guarantee that $\pi_u(t,x,x',y,y')\;\propto\;{Q_u(y,y')}$.

  We therefore have that
  \begin{equation*}
    \Prob{V^*_t\vert\Hist_t}=\Expect{\frac{\Prob{P(\rdm{y})\vert\Hist_t}}{\pi(\rdm{y}\vert\Hist_t)}\cdot\Indicator{\obs(P(\rdm{y}))=V^*_t}},
  \end{equation*}
  the expectation being taken with respect to the random process $\rdm{y}_t$.
  Here, by definition,
  \begin{equation*}
    \pi(\rdm{y}\vert\Hist_t)=\prod_{e\in\event{\Hist}{t}}{\pi_{U_{e}}(e,\leftlim{X}_{e},X_{e},\leftlim{\rdm{y}}_{e},\rdm{y}_{e})}.
  \end{equation*}

  Finally, observe that the condition $\obs(P(\rdm{y}))=V_t$ can be factorized for all obscured genealogies $V_t$.
  In particular, there is a function $\kappa_{V_t}:[0,t]\times\Yspace^2\to\{0,1\}$ such that $\kappa_{V_t}(s,\eta,\eta')=1$ if and only if
  there is $y:\Rp\to\Yspace$ such that $\obs(P(y))=V_t$, $\leftlim{y}_s=\eta$, and $y_s=\eta'$, for all $s\in[0,t]$.
  Thus, conditional on $\Hist_t$, if $\event{P(y)}{t}\subseteq\event{\Hist}{t}$,
  \begin{equation*}
    \Indicator{\obs(P(y))=V^*_t}=\prod_{e\in\event{\Hist}{t}}{\kappa_{V^*_t}(e,\leftlim{y}_e,y_e)}.
  \end{equation*}

  The result then follows from \cref{thm:pruned_lik}.
\end{proof}

%% \AAK{Two approaches possible:}
%% \begin{enumerate}
%% \item Use the filter equations developed in the Appendix to compute $\Expect{\Indicator{\obs(\Prune_t)=\Vis_t}}$.
%%   In effect, we compute $\Prob{\Prune_t}$ and then sum over paintings $y$.
%% \item Change the order of the summation: $\Expect{\Indicator{\obs(\Prune_t)=\Vis_t}}=\Expect{\Expect{\Indicator{\obs(\Prune_t)=\Vis_t}\;\vert\;\Hist_t}}$.
%%   That is, conditional on $\Hist_t$, the likelihood of $\Vis_t$ is sum over all $\Prune_t$ such that $\obs(\Prune_t)=\Vis_t$.
%%   Therefore, any importance sampling scheme for $\Prune_t$ will do.
%%   In particular, we choose a scheme that paints the tree forward in time, driven by a mimic of $\X_t$.
%%   We show that this is equivalent to the specific filter equation.

%%   It may be useful in this to first show that $\Prob{\Prune_t|\Hist_t}$ can be expressed as an expectation over $y$, where $y$ is constrained to be equal $y^{\Prune_t}$.
%%   Then, show that the sum of the $\Prune_t$-constraint indicator functions is equal to the $\Vis_t$-constraint indicator function.
%% \end{enumerate}

%% \begin{figure}
%%   \begin{center}
%%     \input{figs/thm1diag}
%%   \end{center}
%%   \caption{\label{fig:thm1diag}}
%% \end{figure}

%% \begin{center}
%%   \input{figs/triangle1}
%% \end{center}

%% \begin{equation*}
%%   \frac{r!}{(r-s)!}\cdot\frac{(n-\ell)!}{n!}\cdot\frac{(n-r)!}{(n-r-\ell+s)!}
%%   =\frac{r!(n-r)!}{n!}\cdot\frac{(n-\ell)!}{(n-\ell-r+s)!(r-s)!}
%%   =\BinRatio{n}{\ell}{r}{s}
%% \end{equation*}

\paragraph{Integrating out the history: filter equations}

Next, we derive expressions for the likelihood of pruned and obscured genealogies, unconditional on the history.
For this, we use the filter equation technology developed in the Appendix.
In particular, the following theorem follows immediately from \cref{prop:fullfilt}.

\begin{thm}
  Suppose that $P^*_T$ is a pruned genealogy, for $T>0$.
  Let $y^*=Y^{\Prune^*_T}$ and, for $x\in\Xspace$, $u\in\Jumps$, $t\le{T}$, define
  \begin{equation*}
    \phi_u\big(t,x)=\BinRatio{n(x)}{\ell(y^*_t)}{r^u}{s(\yt^*_t,y^*_t)}\,Q_u(\yt^*_t,y^*_t),
  \end{equation*}
  where $Q_u$ is as in \cref{thm:pruned_lik}.
  Suppose $w=w(t,x)$ satisfies the filter equation
  \begin{equation}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}=
      &\sum_u{\int{w(t,x')\,\alpha_u(t,x',x)\,\phi_u(t,x)\,\dd{x'}}}
      -\sum_u{\int{w(t,x)\,\alpha_u(t,x',x)\,\dd{x'}}}\\
      &\ +\sum_{e\in\event{P^*}{T}}{\delta(t,e)\,\left\{\sum_u{\int{w(t,x')\,\alpha_u(t,x',x)\,\phi_u(t,x)\,\dd{x'}}}-w(t,x)\right\}},
    \end{aligned}
  \end{equation}
  with the initial condition $w(0,x)=p_0(x)$.
  Then the likelihood of $P^*_T$ is
  \begin{equation*}
    \lik(P^*_T)=\int{w(T,x)\,\dd{x}}.
  \end{equation*}
\end{thm}

The final result shows how to compute the likelihood of an obscured genealogy.

\begin{thm}
  Let $V^*_T$, $T>0$ be a given obscured genealogy.
  Let $\pi$, $\kappa_{V^*_T}$, and $\rdm{y}_t$ be as in \cref{thm:obsc_lik}.
  For $x,x'\in\Xspace$, $y,y'\in\Yspace$, $u\in\Jumps$, define
  \begin{equation*}
    \begin{gathered}
      \phi_u\big(t,x,y,y')=\BinRatio{n(x)}{\ell(y')}{r^u}{s(y,y')}\,\frac{\kappa_{V^*_T}(t,y,y')}{\pi_u(t,x,x',y,y')},\\
      \beta_u(t,x,x',y,y')=\alpha_u(t,x,x')\,\pi_u(t,x,x',y,y').
    \end{gathered}
  \end{equation*}
  Suppose $w=w(t,x)$ satisfies the filter equation
  \begin{equation}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}(t,x,y)=
      &\sum_{uy'}{\int{w(t,x',y')\,\beta_u(t,x',x,y',y)\,\phi_u(t,x,y',y)\,\dd{x'}}}\\
      &\ -\sum_{uy'}{\int{w(t,x,y)\,\beta_u(t,x,x',y,y')\,\dd{x'}}}\\
      &\ +\sum_{e\in\event{P^*}{T}}{\delta(t,e)\,\left\{\sum_{uy'}{\int{w(t,x')\,\beta_u(t,x',x)\,\phi_u(t,x,y',y)\,\dd{x'}}}-w(t,x)\right\}},
    \end{aligned}
  \end{equation}
  with the initial condition $w(0,x,y)=p_0(x)\,\pi_0(y)$.
  Then the likelihood of $V^*_T$ is
  \begin{equation*}
    \lik(V^*_T)=\sum_y{\int{w(T,x,y)\,\dd{x}}}.
  \end{equation*}
\end{thm}


\section{Examples}

\subsection{SEIRS}

Jumps: $\Jumps=\{\mathrm{Inf},\mathrm{Prog},\mathrm{Recov},\mathrm{Wane},\mathrm{Birth},\mathrm{Death_S},\mathrm{Death_E},\mathrm{Death_I},\mathrm{Death_R},\mathrm{Sample}\}$.

Demes: $\Demes=\{\mathrm{E},\mathrm{I}\}$.

Jump rates:
\begin{itemize}
\item $\alpha_{\mathrm{Inf}}(t,x,x')=\beta(t)\,\frac{x^{\mathrm{S}}x^{\mathrm{I}}}{N(t)}\,\Indicator{x'=x+(-1,1,0,0)}$
\item $\alpha_{\mathrm{Prog}}(x,x')=\rho\,x^{\mathrm{E}}\,\Indicator{x'=x+(0,-1,1,0)}$
\item $\alpha_{\mathrm{Recov}}(x,x')=\gamma\,x^{\mathrm{I}}\,\Indicator{x'=x+(0,0,-1,1)}$
\item $\alpha_{\mathrm{Wane}}(x,x')=\upsilon\,x^{\mathrm{R}}\,\Indicator{x'=x+(1,0,0,-1)}$
\item $\alpha_{\mathrm{Sample}}(t,x,x')=\psi\,x^I\,\Indicator{x'=x}$
\item $\alpha_{\mathrm{Birth}}(t,x,x')=B(t)\,\Indicator{x'=x+(1,0,0,0)}$
\item $\alpha_{\mathrm{Death}_k}(x,x')=\mu\,x^k\,\Indicator{x'^j=x^j-\delta_{jk}}$, $k\in\{\mathrm{S},\mathrm{E},\mathrm{I},\mathrm{R}\}$
\end{itemize}

%% Nonlinear filtering equation:
%% \begin{equation}
%%   \begin{aligned}
%%     \frac{\partial{w}}{\partial{t}}(t,x,y,z)=&B(t)\,\left[\sum_{y'z'}w\big(t,x-(1,0,0,0),y',z'\big)-w\big(t,x,y,z\big)\right]\\
%%     &\qquad+\sum_{k\in\{\mathrm{S},\mathrm{E},\mathrm{I},\mathrm{R}\}}\mu\,(x^k+1)\,w(t,x+\delta_{k})-\mu\,x^k\,w(t,x)\\
%%     &\qquad+\upsilon\,x^{\mathrm{R}}\,w(t,x-(1,0,0,-1))\\
%%   \end{aligned}
%% \end{equation}

\section{Discussion}


\bibliographystyle{preprint}
\bibliography{phylopomp}

\appendix
\setcounter{equation}{0}
\renewcommand{\theequation}{\thesection\arabic{equation}}

\section{Filter equations}

Explicit expressions for the quantities that arise in this paper are not always readily available.
Here, we develop tools for manipulating complex expressions that are otherwise cumbersome.

\begin{defn}
  Suppose $\X_t$ is a continuous-time Markov process with Kolmogorov forward equation (KFE)
  \begin{equation}\label{eq:kfe2}
    \frac{\partial{w}}{\partial{t}}
    =\int{w(t,x')\,\beta(t,x',x)\,\dd{x'}}
    -\int{w(t,x)\,\beta(t,x,x')\,\dd{x'}}.
  \end{equation}
  Suppose that $B(t,x,x')>0$ and $\lambda(t,x)\in\R$ are given measurable functions.
  Let $\mathcal{M}\subseteq\Rp$ be countable such that $\mathcal{M}\cap[0,t]$ is finite for all $t$.
  Let $\nu>0$.
  We say that the equation
  \begin{equation}\label{eq:filtereq}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}
      =&\int{w(t,x')\,\beta(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\beta(t,x,x')\,\dd{x'}}
      -\lambda(t,x)\,w(t,x)\\
      &\quad+\sum_{e\in\mathcal{M}}\delta(t,e)\,\left\{\int{w(t,x')\,\frac{\beta(t,x',x)}{\nu}\,B(t,x',x)\,\dd{x'}}-w(t,x)\right\}
      +\nu\,w(t,x).
    \end{aligned}
  \end{equation}
  is the \emph{filter equation} with \emph{driver} $\X_t$, \emph{boost} $B$, \emph{decay} $\lambda$, and \emph{observed events} $\mathcal{M}$, \emph{relative to} $\nu$.
\end{defn}

\begin{remark}
  \Cref{eq:filtereq} is equivalent to
  \begin{equation*}
    \begin{gathered}
      \frac{\partial{w}}{\partial{t}}
      =\int{w(t,x')\,\beta(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\beta(t,x,x')\,\dd{x'}}
      -(\lambda(t,x)-\nu)\,w(t,x),\quad t\notin{\mathcal{M}},\\
      w(t,x)=\int{\leftlim{w}(t,x')\,\frac{\beta(t,x',x)}{\nu}\,B(t,x',x)\,\dd{x'}},\quad t\in{\mathcal{M}}.
    \end{gathered}
  \end{equation*}
\end{remark}

Filter equations afford a convenient means of computing expectations and likelihoods for pure jump processes.
This is facilitated by the following fact.

\begin{lemma}\label{lemma:filtereq}
  The filter equation \eqref{eq:filtereq} with $\mathcal{M}=\emptyset$ is satisfied by $w(t,x)=\int_0^{\infty}{v\,u(t,x,v)\,\dd{v}}$, where $u$ satisfies the KFE
  \begin{equation}\label{eq:filterlemma}
    \begin{aligned}
      \frac{\partial{u}}{\partial{t}}=&\int{u(t,x',v')\,\beta(t,x',x)\,\delta(v,B(t,x',x)\,v')\,\dd{x'}\,\dd{v'}}\\
      &\qquad-\int{u(t,x,v)\,\beta(t,x,x')\,\delta(v',B(t,x,x')\,v)\,\dd{x'}\,\dd{v'}}
      +\tfrac{\partial}{\partial{v}}\left[\lambda(t,x)\,v\,u(t,x,v)\right],
    \end{aligned}
  \end{equation}
  on the space $\Xspace\times(0,\infty)$.
  Here, $\delta(v,v')$ is the familiar Dirac $\delta$.
\end{lemma}
\begin{proof}
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}
      =&\int{v\,\frac{\partial{u}}{\partial{t}}(t,x,v)\,\dd{v}}\\
      =&\int{v\,u(t,x',v')\,\beta(t,x',x)\,\delta(v,B(t,x',x)v')\,\dd{v}\,\dd{x'}\,\dd{v'}}\\
      &\qquad-\int{v\,u(t,x,v)\,\beta(t,x,x')\,\delta(v',B(t,x,x')v)\,\dd{v}\,\dd{x'}\,\dd{v'}}\\
      &\qquad+\int{v\,\tfrac{\partial}{\partial{v}}\left[\lambda(t,x)\,v\,u(t,x,v)\right]\,\dd{v}}.\\
    \end{aligned}
  \end{equation*}
  Evaluating the first integral with respect to $v$, the second with respect to $v'$, and the third by parts, we obtain
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}
      =&\int{v'\,u(t,x',v')\,\beta(t,x',x)\,B(t,x',x)\,\dd{v'}\,\dd{x'}}
      -\int{v\,u(t,x,v)\,\beta(t,x,x')\,\dd{v}\,\dd{x'}}\\
      &\qquad-\lambda(t,x)\,\int{v\,u(t,x,v)\,\dd{v}},
    \end{aligned}
  \end{equation*}
  which is simplified to obtain \cref{eq:filtereq}.
\end{proof}

\Cref{eq:filterlemma} is recognizable as the KFE of a certain process $(\X_t,\rdm{V}_t)$.
In particular, $\X_t$ is the driver with KFE \eqref{eq:kfe2}.
The $\rdm{V}_t$ is \emph{directed} by $\X_t$ in the sense that $\rdm{V}$ has jumps wherever $\X$ does:
when $\X$ jumps at time $t$ from $x$ to $x'$, $\rdm{V}$ jumps by the multiplicative factor $B(t,x,x')>0$.
Between jumps, $\rdm{V}_t$ decays deterministically and exponentially at rate $\lambda(t,x)$.
If we view $V_t$ as a weight, then \cref{lemma:filtereq} tells us how the $\rdm{V}_t$-weighted average of $\rdm{X}_t$ evolves in time:
this average is simply $\int{w(t,x)\,\dd{x}}$.
This motivates the following result, which shows how filter equations allow one to integrate over random histories.

\begin{prop}\label{prop:zeroboost}
  Suppose $\X_t$ is a non-explosive pure jump process with KFE
  \begin{equation*}
    \begin{gathered}
      \frac{\partial{w}}{\partial{t}}(t,x)=\int{w(t,x')\,\alpha(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\alpha(t,x,x')\,\dd{x'}},\qquad
      w(0,x)=p_0(x).
    \end{gathered}
  \end{equation*}
  Let $\Hist_t$ be its history process.
  Suppose $\rdm{V}_t$ is a real-valued random process such that
  \begin{equation*}
    \Expect{\rdm{V}_t\;\vert\;\Hist_t=H_t}=\prod_{e\in\event{H}{t}}{Q(e,\leftlim{X}_e,X_e)\,B(e,\leftlim{X}_e,X_e)},
  \end{equation*}
  for some given measurable functions $B>{0}$, $Q\in\{0,1\}$.
  If $w$ satisfies the filter equation,
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}(t,x)
      =&\int{w(t,x')\,\alpha(t,x',x)\,Q(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\alpha(t,x,x')\,Q(t,x,x')\,\dd{x'}}\\
      &\qquad-\int{w(t,x)\,\alpha(t,x,x')\,\Big(1-Q(t,x,x')\Big)\,\dd{x'}},
    \end{aligned}
  \end{equation*}
  then $\Expect{\rdm{V}_t}=\int{w(t,x)\,\dd{x}}$.
\end{prop}
\begin{proof}
  Apply \cref{lemma:filtereq} with $\beta(t,x,x')=\alpha(t,x,x')\,Q(t,x,x')$.
\end{proof}

%% An important special case is that of a deterministic driving process.
%% The following result is established by routine calculation.

%% \begin{prop}
%%   Suppose $X:[0,T]\to\Xspace$ is a deterministic, piecewise constant, \cadlag\ function and let $M=\event(X)$.
%%   Then the KFE for $X_t$ is \cref{eq:kfe2} with $\beta(t,x,x')=\sum_{e\in{M}}{\delta(t,e)\delta(x',X_e)}$.
%%   With this driver, the filter equation (\cref{eq:filtereq}) becomes
%%   \begin{equation*}
%%     \begin{gathered}
%%       \frac{\partial{w}}{\partial{t}}=
%%       -\lambda(t,x)\,w(t,x)\quad\text{for}\quad{t\in[0,T]\setminus{M}},\\
%%       w(t,x)=\delta(x,X_t)\,\int{\wt(t,x')\,B(t,x',X_t)\,\dd{x'}}\quad\text{for}\quad{t\in{M}}.
%%     \end{gathered}
%%   \end{equation*}
%% \end{prop}

Filter equations allow us to pass easily between equivalent representations of a process.
For example, an equivalent way of representing $\X_t$ is in terms of its embedded chain and event times.
Let $\rdm{\hat{X}}_k$ be the embedded chain of $\X_t$ and let $\rdm{\hat{T}}_k$ be the point process of its event times.
It is elementary that
\fontsize{10pt}{12pt}\selectfont
\begin{equation*}
  \begin{gathered}
    \Prob{\rdm{\hat{X}}_k=\hat{X}_k\;\Big\vert\;\rdm{\hat{X}}_{k-1}=\hat{X}_{k-1},\rdm{\hat{T}}_k=\hat{T}_k}=\frac{\alpha(\hat{T}_k,\hat{X}_{k-1},\hat{X}_k)}{\int{\alpha(\hat{T}_k,\hat{X}_{k-1},x')\dd{x'}}},\\
    \Prob{\rdm{\hat{T}}_k>\rdm{\hat{T}}_{k-1}+t\;\Big\vert\;\rdm{\hat{X}}_{k-1}=\hat{X}_{k-1},\rdm{\hat{T}}_{k-1}=\hat{T}_{k-1}}=\exp{\left(-\int_{0}^{t}{\int{\alpha(\hat{T}_{k-1}+s,\hat{X}_{k-1},x')\,\dd{x'}}\,\dd{s}}\right)}.
  \end{gathered}
\end{equation*}
\normalfont
Making the definitions,
\begin{equation}\label[pluralequation]{eq:nudefs}
  \begin{gathered}
    A(t,x)=\int{\alpha(t,x,x')\,\dd{x'}}, \qquad
    \pi(t,x,x')=\frac{\alpha(t,x,x')}{A(t,x)},\\
    B(t,x,x')=\frac{A(t,x)}{\nu}, \qquad
    \lambda(t,x)=A(t,x)-\nu,
  \end{gathered}
\end{equation}
we can rewrite the KFE as
\begin{equation}\label{eq:poissdriver}
  \frac{\partial{w}}{\partial{t}}(t,x)=
  \int{w(t,x')\,\nu\,\pi(t,x',x)\,B(t,x,x')\,\dd{x'}}
  -\int{w(t,x)\,\nu\,\pi(t,x,x')\,\dd{x'}}
  -\lambda(t,x)\,w(t,x).
\end{equation}
Here, $\nu>0$ is the intensity of a time-homogenous Poisson process.
Note that $\pi$ is the probability kernel of the embedded chain $\rdm{\hat{X}}$ and $A(t,x)$ is the intensity of the $\rdm{\hat{T}}_k$ process.
We recognize this equation as the filter equation with boost $B$, decay $\lambda$, and driver generated by $\nu\,\pi(t,x,x')$.
It corresponds to the following procedure for simulating $\X_t$:
\begin{compactenum}[(a)]
\item Simulate jump times according to the rate-$\nu$ Poisson process.
\item Simulate the embedded chain $\rdm{\hat{X}}_k$ using the kernel $\pi$.
\item Weight the realization by the product of the $B$ factors.
  Note that this makes the appropriate importance-sampling correction.
\end{compactenum}

By integrating equations of this kind, we effectively compute expectations with respect to the $\X_t$ process.
In particular, we can use \cref{prop:zeroboost} or \cref{eq:poissdriver} to integrate out the unobserved event times $\rdm{\hat{T}}_k$, as well as the unobserved states $\rdm{\hat{X}}_k$.
However, we also need to be able to compute likelihoods for data that include observed event times.
Technically, in such a case, the likelihood is a density on the cone $\mathbb{C}_t=\coprod_{m=0}^{\infty}{\mathbb{C}^m_t}$, where $\mathbb{C}^m_t=\{t\in\halfclosed{0,t}\;\vert\;t_1<t_2<\dots<t_m\}$.
To avoid having to work directly with such complex objects, it is convenient to represent the likelihood as a density with respect to a constant-rate Poisson point process.
This is accomplished as follows.

Suppose the finite set of observed event times $\mathcal{M}\subseteq\halfclosed{0,t}$ is fixed and $|\mathcal{M}|=M$.
Let $\nu>0$ be a constant rate and consider the mixed point process with intensity $\nu+\sum_{e\in\mathcal{M}}\delta(t,e)$.
This point process has events at $e\in\mathcal{M}$ with probability 1 as well as additional, Poisson-distributed random event times.
Using this as the driver in \cref{eq:poissdriver}, we obtain the filter equation
\begin{equation*}
  \begin{aligned}
    \frac{\partial{w}}{\partial{t}}(t,x)=&\int{w(t,x')\,\nu\,\pi(t,x',x)\,B(t,x',x)\,\dd{x'}}-
    \int{w(t,x)\,\nu\,\pi(t,x,x')\,\dd{x'}}
    -\lambda(t,x)\,w(t,x)\\
    &\quad+\sum_{e\in\mathcal{M}}\delta(t,e)\,\left\{\int{w(t,x')\,\pi(t,x',x)\,B(t,x,x')\,\dd{x'}}
    -\int{w(t,x)\,\pi(t,x,x')\,\dd{x'}}\right\}.\\
  \end{aligned}
\end{equation*}
This is equivalent to
\begin{equation}\label[pluralequation]{eq:mixedKFE}
  \begin{aligned}
    \frac{\partial{w}}{\partial{t}}(t,x)
    =&\int{w(t,x')\,\alpha(t,x',x)\,\dd{x'}}
    -\int{w(t,x)\,\alpha(t,x,x')\,\dd{x'}},\quad t\notin{\mathcal{M}},\\
    w(e,x)=&\int{\leftlim{w}(e,x')\,\pi(e,x',x)\,B(e,x',x)\,\dd{x'}}
    =\tfrac{1}{\nu}\,\int{\leftlim{w}(e,x')\,\alpha(e,x',x)\,\dd{x'}},\quad e\in\mathcal{M}.
  \end{aligned}
\end{equation}
\Cref{eq:mixedKFE} is a filter equation relative to $\nu$ with observed events $\mathcal{M}$.
It defines a probability measure $\mu_M$ on $\Xspace\times{\mathbb{C}^M_t}$.
Specifically, $w(t,x)$ is the Radon-Nikodym derivative of $\mu_M$ with respect to the product of the measure on $\Xspace$ and the probability measure of the rate-$\nu$ Poisson process on $\mathbb{C}^M_t$.
That is,
\begin{equation*}
  \mu^M_t(\dd{x}\,\dd{t_1}\,\cdots\,\dd{t_M})=\nu^{M}\,e^{-\nu\,t}\,w(t,x)\,\dd{x}\,\dd{t_1}\,\cdots\,\dd{t_M}.
\end{equation*}

%% We can re-express the history-process density (\cref{eq:Hdens}) using a filter equation as well.
%% Specifically, one can readily verify that if $H_t$ is a history and $w=w(t,x,u)$ satisfies
%% \begin{equation}\label{eq:Hdens_filt}
%%   \begin{aligned}
%%     \frac{\partial{w}}{\partial{t}}(t,x,u)=&-w(t,x,u)\,\sum_{u'}{\int{\alpha_{u'}(t,x,x')\,\dd{x'}}}\\
%%     &\quad+\sum_{e\in\event{H}{t}}{\delta(t,e)\,\left\{\sum_{u'}{\int{w(t,x',u')\,\frac{\alpha_{u'}(t,x',x)}{\nu}\,\dd{x'}}}-w(t,x,u)\right\}},
%%   \end{aligned}
%% \end{equation}
%% then $\pi^H(\dd{t_1}\cdots\dd{t_K})=\nu^K\,e^{-\nu\,t}\,\sum_u{\int{w(t,x,u)\,\dd{x'}}}\,\dd{t_1}\cdots\dd{t_K}$,
%% where $K=\left\vert\event{H}{t}\right\vert$.
%% \Cref{eq:Hdens_filt} is the filter equation, relative to $\nu$, with null driver, boost $B(t,x,x',u,u')=\alpha_{u'}(t,x,x')$, and decay $\lambda(t,x,u)=\sum_{u'}{\int{\alpha_{u'}(t,x,x')\,\dd{x'}}}$.

The same kind of reasoning shows how to compute expectations of a real-valued process $\rdm{V}_t$ directed by $\X_t$.
As before, we will express the expectation $\Expect{\rdm{V}_t}$ as a density with respect to the rate-$\nu$ Poisson point process.

\begin{prop}\label{prop:fullfilt}
  Suppose $\X_t$ is a continuous-time Markov process with state space $\Xspace$ and KFE
  \begin{equation*}
    \frac{\partial{w}}{\partial{t}}(t,x)=\int{w(t,x')\,\alpha(t,x',x)\,\dd{x'}}
    -\int{w(t,x)\,\alpha(t,x,x')\,\dd{x'}}.
  \end{equation*}
  Let $\Hist_t$ be its history process.
  Suppose that $\mathcal{M}\subseteq\halfclosed{0,t}$, $|\mathcal{M}|=M$, is a fixed, finite set of observed event times.
  Let $B>{0}$ and $Q\in\{0,1\}$ be given measurable functions.
  Let $\rdm{V}_t$ be a non-negative real-valued random process such that
  \begin{equation*}
    \Expect{\rdm{V}_t\;\vert\;\Hist_t}=\prod_{e\in{\event{\Hist}{t}}}{Q(e,\leftlim{X}_e,X_e)\,B(e,\leftlim{X}_e,X_e)}.
  \end{equation*}
  Let $\nu>0$ be a fixed rate and let $\X'_t$ be the pure-jump process generated by the rates $\alpha(t,x,x')\,Q(t,x,x')$.
  Suppose that $w(t,x)$ satisfies the filter equation (relative to $\nu$) with driver $\X'_t$, boost $B'$, observation events $\mathcal{M}$, and decay
  \begin{equation*}
    \lambda(t,x)=\int{\alpha(t,x,x')\,\left(1-Q(t,x,x')\right)\,\dd{x'}}.
  \end{equation*}
  Then the measure $\Expect{\rdm{V}_t}$ has density $\int{w(t,x)\,\dd{x}}$ with respect to the rate-$\nu$ Poisson point process.
\end{prop}
\begin{proof}
  We rewrite the filter equation of \cref{prop:zeroboost} using the rate-$\nu$ Poisson driver:
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}(t,x)
      =&\int{w(t,x')\,\alpha(t,x',x)\,Q(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\alpha(t,x,x')\,\dd{x'}}\\
      =&\int{w(t,x')\,\nu\,\pi(t,x',x)\,\frac{A(t,x')\,B(t,x',x)}{\nu}\,\dd{x'}}
      -\int{w(t,x)\,\nu\,\pi(t,x,x')\,\dd{x'}}\\
      &\quad-\lambda(t,x)\,w(t,x),
    \end{aligned}
  \end{equation*}
  where
  \begin{equation*}
    \begin{gathered}
      A(t,x)=\int{\alpha(t,x,x')\,Q(t,x,x')\dd{x'}}, \qquad
      \lambda(t,x)=\int{\alpha(t,x,x')\,\dd{x'}}-\nu,\\
      \pi(t,x,x')=\frac{\alpha(t,x,x')\,Q(t,x,x')}{A(t,x)}.
    \end{gathered}
  \end{equation*}
  Replacing the Poisson driver with $\nu+\sum_{e\in\mathcal{M}}{\delta(t,e)}$ as before, we have
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}(t,x)
      =&\int{w(t,x')\,\nu\,\pi(t,x',x)\,C(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\nu\,\pi(t,x,x')\,\dd{x'}}-\lambda(t,x)\,w(t,x)\\
      &\quad+\sum_{e\in\mathcal{M}}{\delta(t,e)\,\left\{\int{w(t,x')\,\pi(t,x',x')\,C(t,x',x)\,\dd{x'}}
        -\int{w(t,x)\,\pi(t,x,x')\,\dd{x'}}\right\}},\\
    \end{aligned}
  \end{equation*}
  where
  \begin{equation*}
    C(t,x,x')=\frac{A(t,x)\,B(t,x,x')}{\nu}.
  \end{equation*}
  This simplifies to
  \begin{equation*}
    \begin{aligned}
      \frac{\partial{w}}{\partial{t}}(t,x)
      =&\int{w(t,x')\,\alpha(t,x',x)\,Q(t,x',x)\,B(t,x',x)\,\dd{x'}}
      -\int{w(t,x)\,\alpha(t,x,x')\,Q(t,x,x')\,\dd{x'}}\\
      &\quad+\int{w(t,x)\,\alpha(t,x,x')\,\left(1-Q(t,x,x')\right)\,\dd{x'}}\\
      &\quad+\sum_{e\in\mathcal{M}}{\delta(t,e)\,\left\{\int{w(t,x')\,\frac{\alpha(t,x',x)\,Q(t,x',x)}{\nu}\,B(t,x',x)\,\dd{x'}}
        -w(t,x)\right\}}.\\
    \end{aligned}
  \end{equation*}
  In the latter, we recognize the filter equation of the statement.
\end{proof}

\end{document}
